"""
å®éªŒé…ç½®æ–‡ä»¶
åŒ…å«RQ2å’ŒRQ4å®éªŒçš„å„ç§é…ç½®å‚æ•°
"""
from dataclasses import dataclass
from typing import Dict, List, Optional


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str
    path: str
    max_length: int = 4096
    device_map: str = "auto"
    torch_dtype: str = "float16"
    trust_remote_code: bool = True


@dataclass
class GenerationConfig:
    """æ–‡æœ¬ç”Ÿæˆé…ç½®"""
    max_new_tokens: int = 512
    temperature: float = 0.1
    top_p: float = 0.9
    do_sample: bool = True
    repetition_penalty: float = 1.1


@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    longmemeval_path: str = "data/longmemeval_data"  # ç›¸å¯¹è·¯å¾„
    dataset_variant: str = "oracle"  # oracle, small, medium
    max_sessions: int = 20
    max_context_length: int = 8192


@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    output_dir: str = "results"
    save_responses: bool = True
    save_prompts: bool = False
    batch_size: int = 1
    random_seed: int = 42


# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½® - åªä¿ç•™ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
MODELS = {
    # Llama-3.2 3Bæ¨¡å‹é…ç½® (128Kä¸Šä¸‹æ–‡, 12GB GPUå‹å¥½)
    "llama3.2-3b-base": ModelConfig(
        name="llama3.2-3b-base",
        path="meta-llama/Llama-3.2-3B",
    ),
    "llama3.2-3b-instruct": ModelConfig(
        name="llama3.2-3b-instruct", 
        path="meta-llama/Llama-3.2-3B-Instruct",
    ),
    
    # Qwen2.5 3Bæ¨¡å‹é…ç½® (12GB GPUå‹å¥½) - ä½¿ç”¨æœ¬åœ°è·¯å¾„
    "qwen2.5-3b-base": ModelConfig(
        name="qwen2.5-3b-base",
        path="/root/autodl-tmp/models/qwen/Qwen2.5-3B",  # Baseæ¨¡å‹
    ),
    "qwen2.5-3b-instruct": ModelConfig(
        name="qwen2.5-3b-instruct",
        path="/root/autodl-tmp/models/qwen/Qwen2.5-3B-Instruct",  # Instructæ¨¡å‹
    ),
    
    # Mistral-7B-v0.3 æ¨¡å‹é…ç½® (äº‘ä¸Šè¿è¡Œ)
    "mistral-7b-base": ModelConfig(
        name="mistral-7b-base",
        path="mistralai/Mistral-7B-v0.3",
    ),
    "mistral-7b-instruct": ModelConfig(
        name="mistral-7b-instruct",
        path="mistralai/Mistral-7B-Instruct-v0.3",
    ),
}


# RQ2å®éªŒé…ç½® - åªä¿ç•™ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹é…ç½®
RQ2_EXPERIMENT_CONFIGS = {
    # é•¿ä¸Šä¸‹æ–‡æ¨¡å‹é…ç½®
    "long_context": {
        "description": "é•¿ä¸Šä¸‹æ–‡æ¨¡å‹å¯¹æ¯”å®éªŒ - Llama3.2-3B,Qwen2.5-3B, å’ŒMistral-7B-v0.3",
        "model_pairs": [
            ("qwen2.5-3b-base", "qwen2.5-3b-instruct"),
            ("llama3.2-3b-base", "llama3.2-3b-instruct"),
            ("mistral-7b-base", "mistral-7b-instruct"),
        ],
        "data_config": DataConfig(
            dataset_variant="oracle",
            max_sessions=None,        # ä¸é™åˆ¶ä¼šè¯æ•°
            max_context_length=None   # ä¸é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä½¿ç”¨å®Œæ•´æ•°æ®
        ),
        "generation_config": GenerationConfig(
            temperature=0.1,
            max_new_tokens=200,
            top_p=0.9,
            repetition_penalty=1.1
        ),
        "experiment_config": ExperimentConfig(
            output_dir="results/rq2_long_context",
            save_responses=True,
            save_prompts=True,
            batch_size=1
        )
    },
    
    # å•æ¨¡å‹å¯¹é…ç½® - æ–¹ä¾¿é€‰æ‹©ç‰¹å®šæ¨¡å‹
    "qwen2.5-3b": {
        "description": "Qwen2.5-3B Base vs Instruct å¯¹æ¯”å®éªŒ",
        "model_pairs": [
            ("qwen2.5-3b-base", "qwen2.5-3b-instruct"),
        ],
        "data_config": DataConfig(
            dataset_variant="oracle",
            max_sessions=None,
            max_context_length=None
        ),
        "generation_config": GenerationConfig(
            temperature=0.1,
            max_new_tokens=200,
            top_p=0.9,
            repetition_penalty=1.1
        ),
        "experiment_config": ExperimentConfig(
            output_dir="results/rq2_qwen2.5_3b",
            save_responses=True,
            save_prompts=True,
            batch_size=1
        )
    },
    
    "llama3.2-3b": {
        "description": "Llama-3.2-3B Base vs Instruct å¯¹æ¯”å®éªŒ",
        "model_pairs": [
            ("llama3.2-3b-base", "llama3.2-3b-instruct"),
        ],
        "data_config": DataConfig(
            dataset_variant="oracle",
            max_sessions=None,
            max_context_length=None
        ),
        "generation_config": GenerationConfig(
            temperature=0.1,
            max_new_tokens=200,
            top_p=0.9,
            repetition_penalty=1.1
        ),
        "experiment_config": ExperimentConfig(
            output_dir="results/rq2_llama3.2_3b",
            save_responses=True,
            save_prompts=True,
            batch_size=1
        )
    },
    
    "mistral-7b": {
        "description": "Mistral-7B Base vs Instruct å¯¹æ¯”å®éªŒ (äº‘ä¸Šè¿è¡Œ)",
        "model_pairs": [
            ("mistral-7b-base", "mistral-7b-instruct"),
        ],
        "data_config": DataConfig(
            dataset_variant="oracle",
            max_sessions=None,
            max_context_length=None
        ),
        "generation_config": GenerationConfig(
            temperature=0.1,
            max_new_tokens=200,
            top_p=0.9,
            repetition_penalty=1.1
        ),
        "experiment_config": ExperimentConfig(
            output_dir="results/rq2_mistral_7b",
            save_responses=True,
            save_prompts=True,
            batch_size=1
        )
    }
}


# RQ4å®éªŒé…ç½®
RQ4_EXPERIMENT_CONFIGS = {
    "default": {
        "base_model": "llama2-7b-base",
        "rlhf_model": "llama2-7b-chat", 
        "data_config": DataConfig(
            dataset_variant="oracle",
            max_sessions=25  # çŸ¥è¯†æ›´æ–°å¯èƒ½éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
        ),
        "generation_config": GenerationConfig(
            temperature=0.1,
            max_new_tokens=512
        ),
        "experiment_config": ExperimentConfig(
            output_dir="results/rq4_default"
        )
    },
    
    # äº‘ç«¯å…¨ä¸Šä¸‹æ–‡é…ç½® - ä½¿ç”¨28K tokensè¦†ç›–100%æ•°æ®
    "cloud_full_context": {
        "description": "äº‘ç«¯RTX 4090å®Œæ•´ä¸Šä¸‹æ–‡å®éªŒ - è¦†ç›–100%æ•°æ®(28K tokens)",
        "model_pairs": [
            ("qwen2.5-3b-base", "qwen2.5-3b-instruct"),
            ("llama3.2-3b-base", "llama3.2-3b-instruct"), 
            ("mistral-7b-base", "mistral-7b-instruct"),
        ],
        "data_config": DataConfig(
            dataset_variant="oracle",
            max_sessions=None,
            max_context_length=28000  # è¦†ç›–100%æ•°æ®
        ),
        "generation_config": GenerationConfig(
            temperature=0.1,
            max_new_tokens=100,  # ä¼˜åŒ–è¾“å‡ºé•¿åº¦
            top_p=0.9,
            repetition_penalty=1.1
        ),
        "experiment_config": ExperimentConfig(
            output_dir="results/rq2_cloud_full_context",
            save_responses=True,
            save_prompts=True,
            batch_size=1
        )
    },
}


def get_model_config(model_name: str) -> ModelConfig:
    """è·å–æ¨¡å‹é…ç½®"""
    if model_name not in MODELS:
        raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}. å¯ç”¨æ¨¡å‹: {list(MODELS.keys())}")
    return MODELS[model_name]


def get_rq2_config(config_name: str = "default") -> Dict:
    """è·å–RQ2å®éªŒé…ç½®"""
    if config_name not in RQ2_EXPERIMENT_CONFIGS:
        raise ValueError(f"æœªçŸ¥RQ2é…ç½®: {config_name}. å¯ç”¨é…ç½®: {list(RQ2_EXPERIMENT_CONFIGS.keys())}")
    return RQ2_EXPERIMENT_CONFIGS[config_name]


def get_rq4_config(config_name: str = "default") -> Dict:
    """è·å–RQ4å®éªŒé…ç½®"""
    if config_name not in RQ4_EXPERIMENT_CONFIGS:
        raise ValueError(f"æœªçŸ¥RQ4é…ç½®: {config_name}. å¯ç”¨é…ç½®: {list(RQ4_EXPERIMENT_CONFIGS.keys())}")
    return RQ4_EXPERIMENT_CONFIGS[config_name]


def list_available_configs():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®"""
    print("ğŸ”§ å¯ç”¨é…ç½®:")
    print("\nğŸ“Š æ¨¡å‹é…ç½®:")
    for name, config in MODELS.items():
        print(f"  {name}: {config.path}")
    
    print("\nğŸ” RQ2å®éªŒé…ç½®:")
    for name in RQ2_EXPERIMENT_CONFIGS.keys():
        print(f"  {name}")
    
    print("\nğŸ”„ RQ4å®éªŒé…ç½®:")
    for name in RQ4_EXPERIMENT_CONFIGS.keys():
        print(f"  {name}")


if __name__ == "__main__":
    list_available_configs()
