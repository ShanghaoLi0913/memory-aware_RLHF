#!/usr/bin/env python3
"""
RQ4 å®éªŒï¼šKnowledge Updateï¼ˆKUï¼‰ä¸‹çš„æ›´æ–°ä¸€è‡´æ€§ä¸æ‹’ç­”è¡Œä¸º
-----------------------------------------------------------------

ç ”ç©¶é—®é¢˜ï¼šåœ¨åŒ…å«äº‹å®æ›´æ–°çš„ä¸Šä¸‹æ–‡ä¸­ï¼ŒRLHF æ¨¡å‹æ˜¯å¦ä¸èƒ½å‡†ç¡®ä½¿ç”¨æœ€æ–°ä¿¡æ¯ï¼Œ
è€Œæ›´å¯èƒ½å›ç­”æ—§ä¿¡æ¯æˆ–æ‹’ç­”ï¼Ÿ

æŒ‡æ ‡ï¼š
- Update Consistency (UC): ä½¿ç”¨æœ€æ–°äº‹å®å›ç­”çš„æ¯”ä¾‹
- Stale Answer Rate (SAR): ä½¿ç”¨æ—§äº‹å®å›ç­”çš„æ¯”ä¾‹
- Overall Refusal Rate (ORR): åœ¨åº”ç­”å‹ KUï¼ˆä¸å« _absï¼‰ä¸­é€‰æ‹©æ‹’ç­”çš„æ¯”ä¾‹

å®ç°è¯´æ˜ï¼š
- æ•°æ®æºï¼šLongMemEvalï¼ˆJSONï¼‰ï¼Œè‡ªåŠ¨æå– KU å€™é€‰ï¼ˆæ»¡è¶³æœ‰ä¸¤ä¸ª haystack_sessionsï¼Œ
  ç¬¬ä¸€æ®µå«æ—§ç­”æ¡ˆã€ç¬¬äºŒæ®µå«æ–°ç­”æ¡ˆï¼‰ï¼Œå¹¶ä»¥ question_id æ˜¯å¦åŒ…å« "_abs" æ ‡è®°åº”æ‹’ç­”æ ·æœ¬ã€‚
- æ¨ç†ï¼šä¸ RQ2 ä¸€è‡´çš„æœ¬åœ° HF æ¨¡å‹åŠ è½½é€»è¾‘ï¼ˆfp16, device_map="auto"ï¼‰ã€‚
- æ‹’ç­”æ£€æµ‹ï¼šå¤ç”¨ utils/refusal_detector.RefusalDetectorã€‚
- ç»Ÿè®¡ï¼šUC/SAR ä½¿ç”¨ McNemarï¼ˆè¿ç»­æ€§ä¿®æ­£ï¼‰ï¼›ORR ç»™å‡º Wilson 95% CIã€‚
"""

from __future__ import annotations

import json
import os
import math
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from utils.refusal_detector import RefusalDetector
from experiments.config import get_model_config


@dataclass
class RQ4Config:
    base_model_name: str
    rlhf_model_name: str
    longmemeval_path: str = "data/longmemeval_data/longmemeval_oracle.json"
    output_dir: str = "results/rq4_knowledge_update"
    temperature: float = 0.0
    top_p: float = 1.0
    max_new_tokens: int = 256
    max_sessions: Optional[int] = None  # é™åˆ¶æ¯æ¡æ ·æœ¬æ‹¼æ¥çš„ä¼šè¯è½®æ•°
    save_responses: bool = True
    quick_test: bool = False  # ä»…å–å‰è‹¥å¹² KU æ ·æœ¬


def _safe_mkdir(path: str) -> None:
    Path(path).mkdir(parents=True, exist_ok=True)


def _current_ts() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def _normalize_text(text: str) -> str:
    if text is None:
        return ""
    return "".join(ch.lower() for ch in text if ch.isalnum())


def construct_prompt(sessions: List[Dict[str, Any]], question: str, max_sessions: Optional[int]) -> str:
    if max_sessions is not None and max_sessions > 0:
        sessions = sessions[:max_sessions]
    history_lines: List[str] = []

    def add_entry(entry: Any) -> None:
        if isinstance(entry, dict):
            content = entry.get("content", "")
            role = entry.get("role", "assistant")
            if content:
                history_lines.append(f"{role}: {content}")
        elif isinstance(entry, list):
            for e in entry:
                add_entry(e)
        else:
            # Fallback to string
            text = str(entry)
            if text:
                history_lines.append(f"assistant: {text}")

    for s in sessions:
        add_entry(s)
    history = "\n\n".join(history_lines)
    return (
        "You are given a userâ€“assistant chat history.\n"
        "Answer the userâ€™s last question only using the information in the history.\n"
        "If the history does not contain enough information, say you cannot answer.\n\n"
        f"History:\n{history}\n\n"
        f"Question: {question}\n"
        "Answer:"
    )


def load_model(model_name_or_path: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    try:
        cfg = get_model_config(model_name_or_path)
        model_path = cfg.path
        print(f"ğŸ“ ä½¿ç”¨é…ç½®è·¯å¾„: {model_path}")
    except Exception:
        model_path = model_name_or_path
        print(f"âš ï¸ é…ç½®ä¸­æœªæ‰¾åˆ° {model_name_or_path}ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„: {model_path}")

    tok = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    return model, tok


@torch.inference_mode()
def generate(model, tokenizer, prompt: str, temperature: float, top_p: float, max_new_tokens: int) -> str:
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    gen_ids = model.generate(
        **inputs,
        do_sample=temperature > 0,
        temperature=temperature,
        top_p=top_p,
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)
    # Return only the suffix after prompt
    if text.startswith(prompt):
        return text[len(prompt):].strip()
    return text.strip()


def mcnemar_with_correction(b: int, c: int) -> Dict[str, Any]:
    # ç»Ÿè®¡é‡ with continuity correction
    total = b + c
    if total == 0:
        return {"statistic": 0.0, "p_value": 1.0}
    chi2 = (abs(b - c) - 1) ** 2 / total
    try:
        from scipy.stats import chi2 as _chi2
        p_value = 1 - _chi2.cdf(chi2, df=1)
    except Exception:
        # df=1: survival = erfc(sqrt(x/2))
        p_value = math.erfc(math.sqrt(max(chi2, 0.0) / 2.0))
    return {"statistic": float(chi2), "p_value": float(p_value)}


def wilson_ci(k: int, n: int, z: float = 1.96) -> Tuple[float, float, float]:
    if n <= 0:
        return 0.0, 0.0, 0.0
    phat = k / n
    denom = 1 + z * z / n
    center = (phat + (z * z) / (2 * n)) / denom
    half = (z / denom) * math.sqrt((phat * (1 - phat) / n) + (z * z) / (4 * n * n))
    lo, hi = max(0.0, center - half), min(1.0, center + half)
    return phat, lo, hi


def _entry_has_answer(entry: Any) -> bool:
    # entry can be dict, list, or other
    if isinstance(entry, dict):
        return bool(entry.get("has_answer", False))
    if isinstance(entry, list):
        return any(_entry_has_answer(e) for e in entry)
    return False


def extract_ku_samples(dataset: List[Dict[str, Any]], limit: Optional[int] = None) -> List[Dict[str, Any]]:
    ku_items: List[Dict[str, Any]] = []
    for item in dataset:
        sessions = item.get("haystack_sessions", [])
        if not isinstance(sessions, list) or len(sessions) < 2:
            continue
        # è¦æ±‚ç¬¬ä¸€æ®µå«æ—§ç­”æ¡ˆå€™é€‰ï¼Œç¬¬äºŒæ®µå«æ–°ç­”æ¡ˆå€™é€‰ï¼ˆé€šè¿‡ has_answer æ ‡è®°ï¼‰
        old_has = _entry_has_answer(sessions[0])
        new_has = _entry_has_answer(sessions[1])
        if not (old_has or new_has):
            continue
        # å¿…é¡»å­˜åœ¨æ ‡å‡†ç­”æ¡ˆå­—æ®µä¾›æ–°ç­”æ¡ˆåŒ¹é…
        if "answer" not in item:
            continue
        ku_items.append(item)
        if limit is not None and len(ku_items) >= limit:
            break
    return ku_items


def classify_response(resp: str, new_answer: str, old_candidates: List[str], detector: RefusalDetector) -> str:
    # å…ˆæ‹’ç­”æ£€æµ‹
    is_refusal, _ = detector.detect_refusal(resp or "")
    if is_refusal:
        return "refusal"

    r_norm = _normalize_text(resp)
    new_norm = _normalize_text(new_answer)

    new_hit = bool(new_norm and new_norm in r_norm)
    old_hit = False
    for oc in old_candidates:
        if not oc:
            continue
        ocn = _normalize_text(oc)
        if ocn and ocn in r_norm:
            old_hit = True
            break

    # è‹¥æ–°æ—§åŒæ—¶å‘½ä¸­ï¼Œå½’ä¸º otherï¼Œç”±äººå·¥å¤æ ¸
    if new_hit and old_hit:
        return "other"
    if new_hit:
        return "new"
    if old_hit:
        return "old"
    # éƒ½æœªå‘½ä¸­
    return "other"


def run_rq4_experiment(config: RQ4Config) -> Dict[str, Any]:
    _safe_mkdir(config.output_dir)

    # åŠ è½½æ•°æ®
    with open(config.longmemeval_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)

    # æŠ½å– KU å­é›†ï¼ˆå¯å›ç­”+_absæ··å…¥ï¼Œåç»­æ‹†åˆ†ï¼‰
    limit = 15 if config.quick_test else None
    ku_items = extract_ku_samples(dataset, limit=limit)

    # æ ‡è®° ABSï¼ˆåº”æ‹’ç­”ï¼‰
    for it in ku_items:
        qid = str(it.get("question_id", ""))
        it["is_abs"] = ("_abs" in qid)

    # å‡†å¤‡æ¨¡å‹
    base_model, base_tok = load_model(config.base_model_name)
    rlhf_model, rlhf_tok = load_model(config.rlhf_model_name)
    detector = RefusalDetector()

    base_results: List[Dict[str, Any]] = []
    rlhf_results: List[Dict[str, Any]] = []

    def infer_one(model, tok, item) -> Dict[str, Any]:
        prompt = construct_prompt(item["haystack_sessions"], item.get("question", ""), config.max_sessions)
        out = generate(model, tok, prompt, config.temperature, config.top_p, config.max_new_tokens)
        return {
            "question_id": item.get("question_id"),
            "is_abs": item.get("is_abs", False),
            "response": out
        }

    # é€æ¡æ¨ç†ï¼ˆå¹¶å‘=1ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
    for item in ku_items:
        base_results.append(infer_one(base_model, base_tok, item))
        rlhf_results.append(infer_one(rlhf_model, rlhf_tok, item))

    # ç»Ÿè®¡ï¼šéœ€å‡†å¤‡æ–°æ—§ç­”æ¡ˆå€™é€‰
    uc_base = 0
    uc_rlhf = 0
    sar_base = 0
    sar_rlhf = 0
    orr_base = 0
    orr_rlhf = 0

    # old candidates: ä» sessions[0] çš„ content ä¸­ç®€å•æŠ½å–â€”â€”æ­¤å¤„é‡‡ç”¨ç®€åŒ–ç­–ç•¥ï¼š
    # ä½¿ç”¨ item["answer"] ä½œä¸ºæ–°ç­”æ¡ˆï¼Œæ—§ç­”æ¡ˆå€™é€‰ä»ç¬¬ä¸€æ®µä¸­å¯»æ‰¾é«˜ç½®ä¿¡ has_answer çš„æ¶ˆæ¯å†…å®¹é›†åˆã€‚
    # è‹¥æ— ç»“æ„åŒ–æ—§ç­”æ¡ˆï¼Œç»Ÿè®¡æ—¶ä»…ä»¥æ˜¯å¦å‘½ä¸­æ–°ç­”æ¡ˆ/æ‹’ç­”ä½œä¸ºä¸»å£å¾„ï¼Œold ä¸ºä¿å®ˆåŒ¹é…ã€‚
    annotated: List[Dict[str, Any]] = []

    for item, b, r in zip(ku_items, base_results, rlhf_results):
        sessions = item.get("haystack_sessions", [])
        question = item.get("question", "")
        new_answer = item.get("answer", "")
        old_candidates: List[str] = []
        if sessions and len(sessions) >= 1:
            # æ”¶é›†æ‰€æœ‰ has_answer=true çš„å†…å®¹ä½œä¸ºæ—§å€™é€‰ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            all_has_answer_contents = []
            def collect_all_has_answer(entry: Any, acc: List[str]):
                if isinstance(entry, dict):
                    if entry.get("has_answer", False):
                        acc.append(entry.get("content", ""))
                elif isinstance(entry, list):
                    for e in entry:
                        collect_all_has_answer(e, acc)
            
            # æ”¶é›†æ‰€æœ‰ä¼šè¯ä¸­çš„ has_answer å†…å®¹
            for session in sessions:
                collect_all_has_answer(session, all_has_answer_contents)
            
            # æ’é™¤æœ€åä¸€ä¸ªï¼ˆåº”è¯¥æ˜¯æ–°ç­”æ¡ˆï¼‰
            if len(all_has_answer_contents) > 1:
                old_candidates.extend(all_has_answer_contents[:-1])
            elif len(all_has_answer_contents) == 1:
                # å¦‚æœåªæœ‰ä¸€ä¸ªï¼Œè¯´æ˜æ²¡æœ‰æ—§ç­”æ¡ˆï¼Œä¿æŒç©ºåˆ—è¡¨
                pass
        # åˆ†ç±»
        base_cls = classify_response(b["response"], new_answer, old_candidates, detector)
        rlhf_cls = classify_response(r["response"], new_answer, old_candidates, detector)

        # ç´¯åŠ  UC/SARï¼ˆä»…åœ¨åº”ç­”å‹æ ·æœ¬ï¼Œé _absï¼‰
        if not item.get("is_abs", False):
            if base_cls == "new":
                uc_base += 1
            if rlhf_cls == "new":
                uc_rlhf += 1
            if base_cls == "old":
                sar_base += 1
            if rlhf_cls == "old":
                sar_rlhf += 1
            if base_cls == "refusal":
                orr_base += 1
            if rlhf_cls == "refusal":
                orr_rlhf += 1

        annotated.append({
            "question_id": item.get("question_id"),
            "is_abs": item.get("is_abs", False),
            "question": question,
            "new_answer": new_answer,
            "old_candidates": old_candidates,
            "base": {"class": base_cls, "response": b["response"]},
            "rlhf": {"class": rlhf_cls, "response": r["response"]},
        })

    # åˆ†æ¯ï¼ˆåº”ç­”å‹ KUï¼‰
    denom_ku = sum(1 for it in ku_items if not it.get("is_abs", False))

    # æ¯”ä¾‹
    uc_base_rate = (uc_base / denom_ku) if denom_ku else 0.0
    uc_rlhf_rate = (uc_rlhf / denom_ku) if denom_ku else 0.0
    sar_base_rate = (sar_base / denom_ku) if denom_ku else 0.0
    sar_rlhf_rate = (sar_rlhf / denom_ku) if denom_ku else 0.0
    orr_base_rate = (orr_base / denom_ku) if denom_ku else 0.0
    orr_rlhf_rate = (orr_rlhf / denom_ku) if denom_ku else 0.0

    # McNemarï¼šUC ä¸ SAR çš„é…å¯¹äºŒå…ƒç»“æœ
    # b: Base=1, RLHF=0ï¼›c: Base=0, RLHF=1ï¼ˆä»¥â€œä½¿ç”¨æœ€æ–°äº‹å®â€ä¸ºä¾‹ï¼‰
    b_uc = 0
    c_uc = 0
    b_sar = 0
    c_sar = 0
    for a in annotated:
        if a["is_abs"]:
            continue
        base_is_new = (a["base"]["class"] == "new")
        rlhf_is_new = (a["rlhf"]["class"] == "new")
        if base_is_new and not rlhf_is_new:
            b_uc += 1
        if (not base_is_new) and rlhf_is_new:
            c_uc += 1
        base_is_old = (a["base"]["class"] == "old")
        rlhf_is_old = (a["rlhf"]["class"] == "old")
        if base_is_old and not rlhf_is_old:
            b_sar += 1
        if (not base_is_old) and rlhf_is_old:
            c_sar += 1

    m_uc = mcnemar_with_correction(b_uc, c_uc)
    m_sar = mcnemar_with_correction(b_sar, c_sar)

    # Wilson for ORR
    orr_base_w = wilson_ci(orr_base, denom_ku)
    orr_rlhf_w = wilson_ci(orr_rlhf, denom_ku)

    ts = _current_ts()

    # æ„é€ ä¸ RQ2 ä¸€è‡´çš„ç»“æœç›®å½•å‘½åï¼šresults/rq4_<pair_name>
    def _infer_pair_name(base_path: str, rlhf_path: str) -> str:
        lp = f"{base_path}".lower()
        if "qwen2.5-3b" in lp or "qwen2___5-3b" in lp or "qwen2.5_3b" in lp:
            return "qwen2.5_3b"
        if "llama" in lp and "3" in lp:
            return "llama3"
        if "mistral" in lp and "7b" in lp:
            return "mistral-7b"
        # fallback: ä½¿ç”¨baseç›®å½•å
        try:
            return Path(base_path).name.replace(" ", "_")
        except Exception:
            return "custom"

    pair_name = _infer_pair_name(config.base_model_name, config.rlhf_model_name)
    out_dir_name = f"rq4_{pair_name}"
    if config.quick_test:
        out_dir_name = f"{out_dir_name}_quick_test"
    # ä¿å­˜åˆ°æŒ‡å®šæ ¹ç›®å½•ï¼ˆä¸RQ2ä¸€è‡´çš„åˆ†ç»„ç›®å½•ï¼‰ï¼Œä¾‹å¦‚: results/rq4_knowledge_update/rq4_qwen2.5_3b[_quick_test]
    out_dir = os.path.join(config.output_dir, out_dir_name)
    _safe_mkdir(out_dir)

    analysis = {
        "_readme": {
            "description": "RQ4å®éªŒç»“æœåˆ†æ - çŸ¥è¯†æ›´æ–°ä»»åŠ¡ä¸‹çš„æ›´æ–°ä¸€è‡´æ€§ä¸æ‹’ç­”",
            "experiment_type": "Base vs RLHF åœ¨KUä»»åŠ¡",
        },
        "experiment_info": {
            "ku_total": len(ku_items),
            "ku_answerable": denom_ku,
            "base_model": config.base_model_name,
            "rlhf_model": config.rlhf_model_name,
        },
        "ku_analysis": {
            "uc": {
                "base": uc_base_rate,
                "rlhf": uc_rlhf_rate,
                "diff": uc_rlhf_rate - uc_base_rate,
                "mcnemar": {"b": b_uc, "c": c_uc, **m_uc},
            },
            "sar": {
                "base": sar_base_rate,
                "rlhf": sar_rlhf_rate,
                "diff": sar_rlhf_rate - sar_base_rate,
                "mcnemar": {"b": b_sar, "c": c_sar, **m_sar},
            },
            "orr": {
                "base": {"rate": orr_base_rate, "wilson": {"p": orr_base_w[0], "lo": orr_base_w[1], "hi": orr_base_w[2]}},
                "rlhf": {"rate": orr_rlhf_rate, "wilson": {"p": orr_rlhf_w[0], "lo": orr_rlhf_w[1], "hi": orr_rlhf_w[2]}},
                "diff": orr_rlhf_rate - orr_base_rate,
            },
        },
    }

    # ä¿å­˜åˆ†æï¼ˆç²¾ç®€ç‰ˆï¼‰
    analysis_path = os.path.join(out_dir, f"rq4_analysis_{ts}.json")
    with open(analysis_path, "w", encoding="utf-8") as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)

    # ç”Ÿæˆå¸¦æ³¨é‡Šçš„åˆ†æï¼ˆä¸RQ2é£æ ¼ä¸€è‡´ï¼šåœ¨ä¸»è¦å­—æ®µä¸‹åŠ å…¥ _notes è§£é‡Šï¼‰
    annotated_analysis = json.loads(json.dumps(analysis))  # æ·±æ‹·è´
    annotated_analysis.setdefault("_readme", {}).setdefault("field_explanations", {
        "experiment_info": "å®éªŒåŸºæœ¬ä¿¡æ¯ï¼ˆæ ·æœ¬è§„æ¨¡ã€æ¨¡å‹è·¯å¾„ï¼‰",
        "ku_analysis": "KU æŒ‡æ ‡åˆ†æï¼ˆUC/SAR/ORR ä¸ç»Ÿè®¡ï¼‰",
    })
    annotated_analysis.setdefault("experiment_info", {}).setdefault("_notes", {
        "ku_total": "KU å­é›†æ ·æœ¬æ€»æ•°ï¼ˆå« _absï¼‰",
        "ku_answerable": "åº”ç­”å‹ KU åˆ†æ¯ï¼ˆæ’é™¤ _absï¼‰",
        "base_model": "åŸºç¡€æ¨¡å‹è·¯å¾„",
        "rlhf_model": "RLHF æ¨¡å‹è·¯å¾„",
    })
    ka = annotated_analysis.setdefault("ku_analysis", {})
    ka.setdefault("uc", {}).setdefault("_notes", {
        "base": "Base ä½¿ç”¨æœ€æ–°äº‹å®çš„æ¯”ä¾‹",
        "rlhf": "RLHF ä½¿ç”¨æœ€æ–°äº‹å®çš„æ¯”ä¾‹",
        "diff": "å·®å€¼ï¼ˆæ­£å€¼è¡¨ç¤º RLHF æ›´ä¸€è‡´ï¼‰",
        "mcnemar": "é…å¯¹æ£€éªŒï¼ˆb/c ä¸ p å€¼ï¼‰",
    })
    ka.setdefault("sar", {}).setdefault("_notes", {
        "base": "Base ä½¿ç”¨æ—§äº‹å®çš„æ¯”ä¾‹",
        "rlhf": "RLHF ä½¿ç”¨æ—§äº‹å®çš„æ¯”ä¾‹",
        "diff": "å·®å€¼ï¼ˆæ­£å€¼è¡¨ç¤º RLHF æ›´å®¹æ˜“ç­”æ—§ï¼‰",
        "mcnemar": "é…å¯¹æ£€éªŒï¼ˆb/c ä¸ p å€¼ï¼‰",
    })
    ka.setdefault("orr", {}).setdefault("_notes", {
        "base": "Base æ‹’ç­”æ¯”ä¾‹ï¼ˆKU åº”ç­”åˆ†æ¯ï¼‰",
        "rlhf": "RLHF æ‹’ç­”æ¯”ä¾‹ï¼ˆKU åº”ç­”åˆ†æ¯ï¼‰",
        "diff": "å·®å€¼ï¼ˆæ­£å€¼è¡¨ç¤º RLHF æ‹’ç­”æ›´å¤šï¼‰",
    })
    with open(os.path.join(out_dir, f"rq4_analysis_{ts}_annotated.json"), "w", encoding="utf-8") as f:
        json.dump(annotated_analysis, f, ensure_ascii=False, indent=2)

    # æ‹†åˆ†å¹¶ä¿å­˜ base/rlhf å“åº”æ–‡ä»¶ï¼ˆä¸ RQ2 ä¸€è‡´çš„é£æ ¼ï¼‰
    if config.save_responses:
        base_only = []
        rlhf_only = []
        for a in annotated:
            base_only.append({
                "question_id": a["question_id"],
                "is_abs": a["is_abs"],
                "question": a["question"],
                "new_answer": a["new_answer"],
                "old_candidates": a["old_candidates"],
                "response": a["base"]["response"],
                "class": a["base"]["class"],
            })
            rlhf_only.append({
                "question_id": a["question_id"],
                "is_abs": a["is_abs"],
                "question": a["question"],
                "new_answer": a["new_answer"],
                "old_candidates": a["old_candidates"],
                "response": a["rlhf"]["response"],
                "class": a["rlhf"]["class"],
            })

        with open(os.path.join(out_dir, f"rq4_base_ku_responses_{ts}.json"), "w", encoding="utf-8") as f:
            json.dump(base_only, f, ensure_ascii=False, indent=2)
        with open(os.path.join(out_dir, f"rq4_rlhf_ku_responses_{ts}.json"), "w", encoding="utf-8") as f:
            json.dump(rlhf_only, f, ensure_ascii=False, indent=2)
        # ä¿å­˜é€æ¡å®ä¾‹æ˜ç»†ï¼ˆä¾›äººå·¥å¤æ ¸ï¼Œä¸ä¸ analysis_annotated æ··æ·†ï¼‰
        with open(os.path.join(out_dir, f"rq4_ku_instances_{ts}.json"), "w", encoding="utf-8") as f:
            json.dump(annotated, f, ensure_ascii=False, indent=2)

    print(f"âœ… RQ4 å®éªŒå®Œæˆï¼Œè¾“å‡ºç›®å½•: {out_dir}")
    return analysis