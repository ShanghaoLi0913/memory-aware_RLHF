"""
RQ2å®éªŒæ ¸å¿ƒå¼•æ“: RLHFè¿‡åº¦æ‹’ç­”ç°è±¡åˆ†æ
================================================

ç ”ç©¶é—®é¢˜ (RQ2):
    RLHFæ˜¯å¦åœ¨å¯å›ç­”çš„è®°å¿†æ£€ç´¢åœºæ™¯ä¸­è¿‡äºä¿å®ˆï¼Œå¯¼è‡´é”™è¯¯æ‹’ç­”ï¼Ÿ

åŠŸèƒ½æ¦‚è¿°:
    æœ¬æ–‡ä»¶æ˜¯RQ2å®éªŒçš„æ ¸å¿ƒå®ç°ï¼ŒåŒ…å«å®Œæ•´çš„å®éªŒé€»è¾‘ã€æ¨¡å‹å¯¹æ¯”ã€
    æ‹’ç­”æ£€æµ‹å’Œç»“æœåˆ†æåŠŸèƒ½ã€‚è´Ÿè´£æ‰§è¡ŒåŸºç¡€æ¨¡å‹ä¸RLHFæ¨¡å‹çš„æ‹’ç­”è¡Œä¸ºå¯¹æ¯”å®éªŒã€‚

ä¸»è¦ç»„ä»¶:

1. **æ•°æ®ç»“æ„å®šä¹‰**
   - RQ2ExperimentConfig: å®éªŒå‚æ•°é…ç½®ç±»
   - ModelResponse: æ¨¡å‹å“åº”ç»“æœå°è£…ç±»

2. **RefusalDetector æ‹’ç­”æ£€æµ‹å™¨** [æ ¸å¿ƒç®—æ³•]
   - detect_refusal(): ä¸»æ£€æµ‹æ¥å£ï¼Œæ”¯æŒRoBERTaå’Œè§„åˆ™ä¸¤ç§æ–¹æ³•
   - _detect_with_roberta(): åŸºäºRoBERTa-base-squad2çš„ä¸“ä¸šæ£€æµ‹
   - _detect_with_rules(): æ”¹è¿›çš„è§„åˆ™æ£€æµ‹ç®—æ³• (100%æµ‹è¯•å‡†ç¡®ç‡)
   - get_detection_method(): è¿”å›å½“å‰ä½¿ç”¨çš„æ£€æµ‹æ–¹æ³•

3. **RQ2Experimenter å®éªŒæ‰§è¡Œå™¨** [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘]
   - load_model(): åŠ è½½Hugging Faceæ¨¡å‹ (åŸºç¡€æ¨¡å‹/RLHFæ¨¡å‹)
   - create_prompt(): ç”Ÿæˆæ ‡å‡†åŒ–çš„å¯¹è¯å†å²æç¤º
   - generate_response(): æ‰§è¡Œæ¨¡å‹æ¨ç†ç”Ÿæˆå“åº”
   - evaluate_model(): è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ‹’ç­”è¡Œä¸º
   - compare_models(): å¯¹æ¯”åŸºç¡€æ¨¡å‹vs RLHFæ¨¡å‹çš„æ‹’ç­”å·®å¼‚
   - analyze_responses(): ç»Ÿè®¡åˆ†ææ‹’ç­”ç‡ã€ç½®ä¿¡åº¦ã€é—®é¢˜ç±»å‹åˆ†å¸ƒ
   - calculate_significance(): è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§ (å¡æ–¹æ£€éªŒ)
   - save_results(): ä¿å­˜å®éªŒç»“æœåˆ°JSONæ–‡ä»¶
   - print_summary(): è¾“å‡ºå®éªŒç»“æœæ‘˜è¦æŠ¥å‘Š

4. **run_rq2_experiment() ä¸»å®éªŒå‡½æ•°**
   - å®éªŒå…¥å£ç‚¹ï¼Œå¯è¢«å¤–éƒ¨è„šæœ¬è°ƒç”¨
   - è‡ªåŠ¨åˆ›å»ºå®éªŒå™¨å¹¶æ‰§è¡Œå®Œæ•´æµç¨‹

å®éªŒæµç¨‹:
    1. æ•°æ®åŠ è½½ â†’ LongMemEvalæ•°æ®é›†ç­›é€‰RQ2ç›¸å…³å®ä¾‹ (æœ‰è¯æ®ä¸”å¯å›ç­”)
    2. æ¨¡å‹åŠ è½½ â†’ åˆ†åˆ«åŠ è½½åŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹
    3. æ¨ç†ç”Ÿæˆ â†’ å¯¹æ¯ä¸ªå®ä¾‹ç”Ÿæˆæ¨¡å‹å“åº”
    4. æ‹’ç­”æ£€æµ‹ â†’ ä½¿ç”¨ä¼˜åŒ–ç®—æ³•æ£€æµ‹å“åº”æ˜¯å¦ä¸ºæ‹’ç­”
    5. å¯¹æ¯”åˆ†æ â†’ è®¡ç®—æ‹’ç­”ç‡å·®å¼‚å’Œç»Ÿè®¡æ˜¾è‘—æ€§
    6. ç»“æœä¿å­˜ â†’ ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’ŒåŸå§‹æ•°æ®

æ”¯æŒçš„æ¨¡å‹:
    - meta-llama/Llama-2-7b-hf (åŸºç¡€)
    - meta-llama/Llama-2-7b-chat-hf (RLHF)
    - meta-llama/Llama-2-13b-hf/chat-hf
    - mistralai/Mistral-7B-v0.1/Instruct-v0.2
    - å…¶ä»–Hugging Faceå…¼å®¹æ¨¡å‹

è¾“å‡ºç»“æœ:
    - æ‹’ç­”ç‡å¯¹æ¯” (åŸºç¡€ vs RLHF)
    - æŒ‰é—®é¢˜ç±»å‹çš„åˆ†æ
    - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    - åŸå§‹å“åº”æ•°æ® (JSONæ ¼å¼)
    - å®éªŒé…ç½®è®°å½•

ä½¿ç”¨ç¤ºä¾‹:
    ```python
    from experiments.rq2_over_refusal import RQ2ExperimentConfig, run_rq2_experiment
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = RQ2ExperimentConfig(
        base_model_name="meta-llama/Llama-2-7b-hf",
        rlhf_model_name="meta-llama/Llama-2-7b-chat-hf",
        output_dir="results/my_experiment"
    )
    
    # è¿è¡Œå®éªŒ
    results = run_rq2_experiment(config)
    ```

ä¾èµ–è¦æ±‚:
    - PyTorch >= 2.0.0
    - Transformers >= 4.30.0
    - LongMemEvalæ•°æ®é›†
    - GPUæ¨è (CPUä¹Ÿå¯è¿è¡Œä½†è¾ƒæ…¢)

æ³¨æ„äº‹é¡¹:
    - å¤§æ¨¡å‹æ¨ç†éœ€è¦è¾ƒå¤šGPUæ˜¾å­˜
    - å®Œæ•´å®éªŒå¯èƒ½éœ€è¦æ•°å°æ—¶è¿è¡Œæ—¶é—´
    - ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ä¾èµ–äºè¶³å¤Ÿçš„æ ·æœ¬æ•°é‡
"""
import json
import os
import torch
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm

from data.longmemeval_loader import LongMemEvalLoader, LongMemEvalInstance
from utils.refusal_detector import RefusalDetector


@dataclass
class RQ2ExperimentConfig:
    """RQ2å®éªŒé…ç½®"""
    # æ¨¡å‹é…ç½®
    base_model_name: str = "Qwen/Qwen2.5-3B"  # åŸºç¡€æ¨¡å‹
    rlhf_model_name: str = "Qwen/Qwen2.5-3B-Instruct"  # RLHFæ¨¡å‹
    
    # æ•°æ®é…ç½®
    longmemeval_path: str = "data/longmemeval_data/longmemeval_oracle.json"
    max_sessions: Optional[int] = None  # æœ€å¤§ä¼šè¯æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    max_instances: Optional[int] = None  # æœ€å¤§å®ä¾‹æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    max_tokens: int = 28000   # RTX 4070ä¸´æ—¶è®¾ç½®ï¼Œåç»­äº‘ç«¯ç”¨28000
    
    # å®éªŒå‚æ•°
    temperature: float = 0.1
    top_p: float = 0.9
    max_new_tokens: int = 100  # å‡å°‘è¾“å‡ºé•¿åº¦ï¼Œæ‹’ç­”æ£€æµ‹ä¸éœ€è¦å¤ªé•¿å“åº”
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "results/rq2_qwen2.5_3b"
    save_responses: bool = True


@dataclass
class ModelResponse:
    """æ¨¡å‹å“åº”ç»“æœ"""
    question_id: str
    model_name: str
    question: str
    context_length: int
    response: str
    is_refusal: bool
    refusal_confidence: float
    has_evidence: bool
    answer_quality: Optional[float] = None



class RQ2Experimenter:
    """RQ2å®éªŒæ‰§è¡Œå™¨"""
    
    def __init__(self, config: RQ2ExperimentConfig):
        self.config = config
        self.refusal_detector = RefusalDetector()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ•°æ®
        self.loader = LongMemEvalLoader(config.longmemeval_path)
        
        # åŠ è½½IEå’ŒABSä¸¤ä¸ªå­é›†
        self.ie_instances = self.loader.get_rq2_instances()  # Information Extraction - åº”è¯¥å›ç­”
        self.abs_instances = self.loader.get_abstention_instances()  # Abstention - åº”è¯¥æ‹’ç­”
        
        # åº”ç”¨å®ä¾‹æ•°é‡é™åˆ¶
        if config.max_instances is not None:
            self.ie_instances = self.ie_instances[:config.max_instances]
            # ABSæ•°é‡è¾ƒå°‘ï¼ŒæŒ‰æ¯”ä¾‹é™åˆ¶
            abs_limit = min(len(self.abs_instances), config.max_instances // 3)
            self.abs_instances = self.abs_instances[:abs_limit]
            print(f"âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šIEé™åˆ¶ä¸ºå‰ {len(self.ie_instances)} ä¸ªï¼ŒABSé™åˆ¶ä¸ºå‰ {len(self.abs_instances)} ä¸ª")
        
        print(f"åŠ è½½äº† {len(self.ie_instances)} ä¸ªIEå®ä¾‹ï¼ˆåº”è¯¥å›ç­”ï¼‰")
        print(f"åŠ è½½äº† {len(self.abs_instances)} ä¸ªABSå®ä¾‹ï¼ˆåº”è¯¥æ‹’ç­”ï¼‰")
    
    def load_model(self, model_name: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç®€åŒ–çš„æ¨¡å‹åŠ è½½é…ç½®
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",  # è®©transformersè‡ªåŠ¨é€‰æ‹©
            trust_remote_code=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        return model, tokenizer
    
    def create_prompt(self, instance: LongMemEvalInstance) -> str:
        """åˆ›å»ºæ¨¡å‹è¾“å…¥æç¤º"""
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        history = self.loader.format_conversation_history(
            instance, max_sessions=self.config.max_sessions
        )
        
        prompt = f"""You are a helpful AI assistant. Below is a conversation history between a user and an assistant, followed by a question. Please answer the question based on the information provided in the conversation history.

Conversation History:
{history}

Question: {instance.question}

Answer:"""
        
        return prompt
    
    def generate_response(self, model: AutoModelForCausalLM, 
                         tokenizer: AutoTokenizer, 
                         prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=self.config.max_tokens
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def evaluate_model(self, model_name: str) -> Tuple[List[ModelResponse], List[ModelResponse]]:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹ï¼Œè¿”å›IEå’ŒABSä¸¤ä¸ªå­é›†çš„å“åº”"""
        print(f"\nå¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
        
        model, tokenizer = self.load_model(model_name)
        
        # è¯„ä¼°IEå®ä¾‹ï¼ˆåº”è¯¥å›ç­”ï¼‰
        ie_responses = []
        for instance in tqdm(self.ie_instances, desc=f"è¯„ä¼° {model_name} - IE"):
            prompt = self.create_prompt(instance)
            response_text = self.generate_response(model, tokenizer, prompt)
            
            is_refusal, refusal_confidence = self.refusal_detector.detect_refusal(
                response=response_text,
                question=instance.question
            )
            
            response = ModelResponse(
                question_id=instance.question_id,
                model_name=model_name,
                question=instance.question,
                context_length=len(prompt),
                response=response_text,
                is_refusal=is_refusal,
                refusal_confidence=refusal_confidence,
                has_evidence=instance.has_evidence_in_context
            )
            ie_responses.append(response)
        
        # è¯„ä¼°ABSå®ä¾‹ï¼ˆåº”è¯¥æ‹’ç­”ï¼‰
        abs_responses = []
        for instance in tqdm(self.abs_instances, desc=f"è¯„ä¼° {model_name} - ABS"):
            prompt = self.create_prompt(instance)
            response_text = self.generate_response(model, tokenizer, prompt)
            
            is_refusal, refusal_confidence = self.refusal_detector.detect_refusal(
                response=response_text,
                question=instance.question
            )
            
            response = ModelResponse(
                question_id=instance.question_id,
                model_name=model_name,
                question=instance.question,
                context_length=len(prompt),
                response=response_text,
                is_refusal=is_refusal,
                refusal_confidence=refusal_confidence,
                has_evidence=False  # ABSå®ä¾‹è®¾è®¡ä¸ºæ— è¯æ®
            )
            abs_responses.append(response)
        
        # æ¸…ç†GPUå†…å­˜
        del model
        torch.cuda.empty_cache()
        
        return ie_responses, abs_responses
    
    def compare_models(self) -> Dict[str, Any]:
        """æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹åœ¨IEå’ŒABSä¸¤ä¸ªå­é›†ä¸Šçš„è¡¨ç°"""
        print("å¼€å§‹RQ2å®éªŒ: æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹çš„æ‹’ç­”è¡Œä¸º")
        
        # è¯„ä¼°åŸºç¡€æ¨¡å‹
        base_ie_responses, base_abs_responses = self.evaluate_model(self.config.base_model_name)
        
        # è¯„ä¼°RLHFæ¨¡å‹
        rlhf_ie_responses, rlhf_abs_responses = self.evaluate_model(self.config.rlhf_model_name)
        
        # åˆ†æç»“æœ
        analysis = self.analyze_rq2_responses(
            base_ie_responses, base_abs_responses,
            rlhf_ie_responses, rlhf_abs_responses
        )
        
        # ä¿å­˜ç»“æœ
        if self.config.save_responses:
            self.save_rq2_results(
                base_ie_responses, base_abs_responses,
                rlhf_ie_responses, rlhf_abs_responses,
                analysis
            )
        
        return analysis
    
    def analyze_rq2_responses(self, 
                             base_ie_responses: List[ModelResponse],
                             base_abs_responses: List[ModelResponse],
                             rlhf_ie_responses: List[ModelResponse],
                             rlhf_abs_responses: List[ModelResponse]) -> Dict[str, Any]:
        """åˆ†æRQ2å®éªŒç»“æœï¼Œè®¡ç®—ORRã€IE-Accã€ABS-LegitRefuseç­‰æŒ‡æ ‡"""
        
        # è®¡ç®—IEå­é›†çš„ORR (Over-Refusal Rate)
        base_ie_refusals = sum(1 for r in base_ie_responses if r.is_refusal)
        rlhf_ie_refusals = sum(1 for r in rlhf_ie_responses if r.is_refusal)
        
        base_orr = base_ie_refusals / len(base_ie_responses) if base_ie_responses else 0
        rlhf_orr = rlhf_ie_refusals / len(rlhf_ie_responses) if rlhf_ie_responses else 0
        
        # è®¡ç®—ABSå­é›†çš„åˆæ³•æ‹’ç­”ç‡ (Legitimate Refusal Rate)
        base_abs_refusals = sum(1 for r in base_abs_responses if r.is_refusal)
        rlhf_abs_refusals = sum(1 for r in rlhf_abs_responses if r.is_refusal)
        
        base_abs_legit_refuse = base_abs_refusals / len(base_abs_responses) if base_abs_responses else 0
        rlhf_abs_legit_refuse = rlhf_abs_refusals / len(rlhf_abs_responses) if rlhf_abs_responses else 0
        
        # è®¡ç®—IEå­é›†ä¸­éæ‹’ç­”å›ç­”çš„æ•°é‡ (ä¸ºåç»­å‡†ç¡®ç‡è®¡ç®—å‡†å¤‡)
        base_ie_non_refusal = [r for r in base_ie_responses if not r.is_refusal]
        rlhf_ie_non_refusal = [r for r in rlhf_ie_responses if not r.is_refusal]
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (McNemar test for paired comparison)
        mcnemar_result = self.calculate_mcnemar_test(base_ie_responses, rlhf_ie_responses)
        
        analysis = {
            'experiment_info': {
                'ie_total_count': len(base_ie_responses),
                'abs_total_count': len(base_abs_responses),
                'base_model': self.config.base_model_name,
                'rlhf_model': self.config.rlhf_model_name
            },
            'orr_analysis': {
                'base_orr': round(base_orr, 4),
                'rlhf_orr': round(rlhf_orr, 4),
                'orr_difference': round(rlhf_orr - base_orr, 4),
                'base_ie_refusals': base_ie_refusals,
                'rlhf_ie_refusals': rlhf_ie_refusals,
                'interpretation': 'RLHFæ›´ä¿å®ˆ' if rlhf_orr > base_orr else 'Baseæ›´ä¿å®ˆ'
            },
            'abs_analysis': {
                'base_abs_legit_refuse': round(base_abs_legit_refuse, 4),
                'rlhf_abs_legit_refuse': round(rlhf_abs_legit_refuse, 4),
                'legit_refuse_difference': round(rlhf_abs_legit_refuse - base_abs_legit_refuse, 4),
                'base_abs_refusals': base_abs_refusals,
                'rlhf_abs_refusals': rlhf_abs_refusals
            },
            'ie_non_refusal_analysis': {
                'base_non_refusal_count': len(base_ie_non_refusal),
                'rlhf_non_refusal_count': len(rlhf_ie_non_refusal),
                'base_non_refusal_rate': round(len(base_ie_non_refusal) / len(base_ie_responses), 4) if base_ie_responses else 0,
                'rlhf_non_refusal_rate': round(len(rlhf_ie_non_refusal) / len(rlhf_ie_responses), 4) if rlhf_ie_responses else 0
            },
            'statistical_tests': {
                'mcnemar_test': mcnemar_result
            },
            'summary': {
                'key_finding': f"RLHFæ¨¡å‹åœ¨IEä¸Šæ‹’ç­”ç‡{'å¢åŠ ' if rlhf_orr > base_orr else 'å‡å°‘'} {abs(rlhf_orr - base_orr):.1%}",
                'over_refusal_evidence': rlhf_orr > base_orr,
                'abs_legitimacy': f"RLHFåœ¨ABSä¸Šåˆæ³•æ‹’ç­”ç‡: {rlhf_abs_legit_refuse:.1%}"
            }
        }
        
        return analysis
    
    def calculate_mcnemar_test(self, base_responses: List[ModelResponse], 
                              rlhf_responses: List[ModelResponse]) -> Dict[str, Any]:
        """è®¡ç®—McNemaræ£€éªŒï¼Œæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹åœ¨ç›¸åŒé—®é¢˜ä¸Šçš„æ‹’ç­”è¡Œä¸ºå·®å¼‚"""
        from scipy.stats import mcnemar
        
        # æ„å»º2x2è¡¨æ ¼ï¼šbase_refuse vs rlhf_refuse
        both_refuse = 0      # ä¸¤ä¸ªéƒ½æ‹’ç­”
        base_only_refuse = 0 # åªæœ‰baseæ‹’ç­”
        rlhf_only_refuse = 0 # åªæœ‰rlhfæ‹’ç­”
        both_answer = 0      # ä¸¤ä¸ªéƒ½å›ç­”
        
        for base_resp, rlhf_resp in zip(base_responses, rlhf_responses):
            if base_resp.is_refusal and rlhf_resp.is_refusal:
                both_refuse += 1
            elif base_resp.is_refusal and not rlhf_resp.is_refusal:
                base_only_refuse += 1
            elif not base_resp.is_refusal and rlhf_resp.is_refusal:
                rlhf_only_refuse += 1
            else:
                both_answer += 1
        
        # McNemaræ£€éªŒçš„2x2è¡¨æ ¼
        contingency_table = [[both_refuse, base_only_refuse],
                           [rlhf_only_refuse, both_answer]]
        
        try:
            result = mcnemar(contingency_table, exact=True)
            return {
                'statistic': float(result.statistic),
                'p_value': float(result.pvalue),
                'significant': result.pvalue < 0.05,
                'contingency_table': contingency_table,
                'interpretation': 'RLHFæ˜¾è‘—æ›´ä¿å®ˆ' if result.pvalue < 0.05 and rlhf_only_refuse > base_only_refuse else 'æ— æ˜¾è‘—å·®å¼‚'
            }
        except Exception as e:
            return {
                'error': str(e),
                'contingency_table': contingency_table,
                'interpretation': 'æ— æ³•è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§'
            }
    
    def analyze_responses(self, base_responses: List[ModelResponse], 
                         rlhf_responses: List[ModelResponse]) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹å“åº”å·®å¼‚ (ä¿ç•™åŸå‡½æ•°ä»¥å…¼å®¹æ—§ä»£ç )"""
        
        # è®¡ç®—æ‹’ç­”ç‡
        base_refusal_rate = sum(r.is_refusal for r in base_responses) / len(base_responses)
        rlhf_refusal_rate = sum(r.is_refusal for r in rlhf_responses) / len(rlhf_responses)
        
        # è®¡ç®—å¹³å‡æ‹’ç­”ç½®ä¿¡åº¦
        base_refusal_conf = np.mean([r.refusal_confidence for r in base_responses])
        rlhf_refusal_conf = np.mean([r.refusal_confidence for r in rlhf_responses])
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
        question_types = {}
        for response in base_responses:
            q_type = next(
                (inst.question_type for inst in self.rq2_instances 
                 if inst.question_id == response.question_id), 
                "unknown"
            )
            if q_type not in question_types:
                question_types[q_type] = {"base": [], "rlhf": []}
            question_types[q_type]["base"].append(response.is_refusal)
        
        for response in rlhf_responses:
            q_type = next(
                (inst.question_type for inst in self.rq2_instances 
                 if inst.question_id == response.question_id), 
                "unknown"
            )
            question_types[q_type]["rlhf"].append(response.is_refusal)
        
        # è®¡ç®—å„ç±»å‹çš„æ‹’ç­”ç‡
        type_analysis = {}
        for q_type, data in question_types.items():
            type_analysis[q_type] = {
                "base_refusal_rate": np.mean(data["base"]) if data["base"] else 0,
                "rlhf_refusal_rate": np.mean(data["rlhf"]) if data["rlhf"] else 0,
                "count": len(data["base"])
            }
        
        analysis = {
            "overall_metrics": {
                "base_model": {
                    "refusal_rate": base_refusal_rate,
                    "avg_refusal_confidence": base_refusal_conf,
                    "total_instances": len(base_responses)
                },
                "rlhf_model": {
                    "refusal_rate": rlhf_refusal_rate,
                    "avg_refusal_confidence": rlhf_refusal_conf,
                    "total_instances": len(rlhf_responses)
                },
                "refusal_rate_increase": rlhf_refusal_rate - base_refusal_rate,
                "confidence_increase": rlhf_refusal_conf - base_refusal_conf
            },
            "by_question_type": type_analysis,
            "rq2_conclusion": {
                "over_refusal_detected": rlhf_refusal_rate > base_refusal_rate,
                "over_refusal_magnitude": rlhf_refusal_rate - base_refusal_rate,
                "statistical_significance": self.calculate_significance(
                    base_responses, rlhf_responses
                )
            }
        }
        
        return analysis
    
    def save_rq2_results(self, 
                        base_ie_responses: List[ModelResponse],
                        base_abs_responses: List[ModelResponse],
                        rlhf_ie_responses: List[ModelResponse],
                        rlhf_abs_responses: List[ModelResponse],
                        analysis: Dict[str, Any]):
        """ä¿å­˜RQ2å®éªŒç»“æœåˆ°æ–‡ä»¶"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜IEå­é›†å“åº”
        base_ie_file = self.output_dir / f"rq2_base_ie_responses_{timestamp}.json"
        rlhf_ie_file = self.output_dir / f"rq2_rlhf_ie_responses_{timestamp}.json"
        
        # ä¿å­˜ABSå­é›†å“åº”
        base_abs_file = self.output_dir / f"rq2_base_abs_responses_{timestamp}.json"
        rlhf_abs_file = self.output_dir / f"rq2_rlhf_abs_responses_{timestamp}.json"
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = self.output_dir / f"rq2_analysis_{timestamp}.json"
        
        # JSONç¼–ç å™¨å¤„ç†NumPyç±»å‹
        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return super().default(obj)
        
        # ä¿å­˜å„ä¸ªæ–‡ä»¶
        with open(base_ie_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in base_ie_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        with open(rlhf_ie_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in rlhf_ie_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
        with open(base_abs_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in base_abs_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
        with open(rlhf_abs_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in rlhf_abs_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        print(f"\nğŸ“Š RQ2å®éªŒç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  Base IEå“åº”: {base_ie_file}")
        print(f"  RLHF IEå“åº”: {rlhf_ie_file}")
        print(f"  Base ABSå“åº”: {base_abs_file}")
        print(f"  RLHF ABSå“åº”: {rlhf_abs_file}")
        print(f"  åˆ†æç»“æœ: {analysis_file}")
        
        # æ‰“å°å®éªŒæ‘˜è¦
        self.print_rq2_summary(analysis)
    
    def print_rq2_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°RQ2å®éªŒç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š RQ2å®éªŒç»“æœæ‘˜è¦: RLHFè¿‡åº¦æ‹’ç­”ç°è±¡åˆ†æ")
        print("="*60)
        
        exp_info = analysis['experiment_info']
        orr_analysis = analysis['orr_analysis']
        abs_analysis = analysis['abs_analysis']
        mcnemar = analysis['statistical_tests']['mcnemar_test']
        summary = analysis['summary']
        
        print(f"ğŸ·ï¸  å®éªŒé…ç½®:")
        print(f"   åŸºç¡€æ¨¡å‹: {exp_info['base_model']}")
        print(f"   RLHFæ¨¡å‹: {exp_info['rlhf_model']}")
        print(f"   IEå®ä¾‹æ•°: {exp_info['ie_total_count']} (åº”è¯¥å›ç­”)")
        print(f"   ABSå®ä¾‹æ•°: {exp_info['abs_total_count']} (åº”è¯¥æ‹’ç­”)")
        
        print(f"\nğŸ“ˆ ORR (Over-Refusal Rate) åˆ†æ:")
        print(f"   Baseæ¨¡å‹ IEæ‹’ç­”ç‡: {orr_analysis['base_orr']:.1%} ({orr_analysis['base_ie_refusals']}/{exp_info['ie_total_count']})")
        print(f"   RLHFæ¨¡å‹ IEæ‹’ç­”ç‡: {orr_analysis['rlhf_orr']:.1%} ({orr_analysis['rlhf_ie_refusals']}/{exp_info['ie_total_count']})")
        print(f"   æ‹’ç­”ç‡å˜åŒ–: {orr_analysis['orr_difference']:+.1%} ({orr_analysis['interpretation']})")
        
        print(f"\nğŸš« ABS (Abstention) åˆæ³•æ‹’ç­”åˆ†æ:")
        print(f"   Baseæ¨¡å‹ ABSæ‹’ç­”ç‡: {abs_analysis['base_abs_legit_refuse']:.1%} ({abs_analysis['base_abs_refusals']}/{exp_info['abs_total_count']})")
        print(f"   RLHFæ¨¡å‹ ABSæ‹’ç­”ç‡: {abs_analysis['rlhf_abs_legit_refuse']:.1%} ({abs_analysis['rlhf_abs_refusals']}/{exp_info['abs_total_count']})")
        print(f"   åˆæ³•æ‹’ç­”ç‡å˜åŒ–: {abs_analysis['legit_refuse_difference']:+.1%}")
        
        print(f"\nğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (McNemar Test):")
        if 'error' not in mcnemar:
            print(f"   æ£€éªŒç»Ÿè®¡é‡: {mcnemar['statistic']:.3f}")
            print(f"   På€¼: {mcnemar['p_value']:.4f}")
            print(f"   æ˜¯å¦æ˜¾è‘— (p<0.05): {'æ˜¯' if mcnemar['significant'] else 'å¦'}")
            print(f"   ç»“è®º: {mcnemar['interpretation']}")
        else:
            print(f"   ç»Ÿè®¡æ£€éªŒå¤±è´¥: {mcnemar['error']}")
        
        print(f"\nğŸ¯ RQ2æ ¸å¿ƒå‘ç°:")
        print(f"   {summary['key_finding']}")
        print(f"   è¿‡åº¦æ‹’ç­”è¯æ®: {'å‘ç°' if summary['over_refusal_evidence'] else 'æœªå‘ç°'}")
        print(f"   {summary['abs_legitimacy']}")
        
        print("="*60)
    
    def calculate_significance(self, base_responses: List[ModelResponse], 
                             rlhf_responses: List[ModelResponse]) -> Dict[str, float]:
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
        from scipy import stats
        
        base_refusals = [r.is_refusal for r in base_responses]
        rlhf_refusals = [r.is_refusal for r in rlhf_responses]
        
        # ä½¿ç”¨å¡æ–¹æ£€éªŒ
        contingency_table = [
            [sum(base_refusals), len(base_refusals) - sum(base_refusals)],
            [sum(rlhf_refusals), len(rlhf_refusals) - sum(rlhf_refusals)]
        ]
        
        chi2, p_value = stats.chi2_contingency(contingency_table)[:2]
        
        return {
            "chi2_statistic": float(chi2),
            "p_value": float(p_value),
            "significant_at_0.05": p_value < 0.05
        }
    
    def save_results(self, base_responses: List[ModelResponse], 
                    rlhf_responses: List[ModelResponse], 
                    analysis: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åŸå§‹å“åº”
        base_file = self.output_dir / f"rq2_base_responses_{timestamp}.json"
        rlhf_file = self.output_dir / f"rq2_rlhf_responses_{timestamp}.json"
        analysis_file = self.output_dir / f"rq2_analysis_{timestamp}.json"
        
        with open(base_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in base_responses], f, indent=2, ensure_ascii=False)
        
        with open(rlhf_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in rlhf_responses], f, indent=2, ensure_ascii=False)
        
        # å®šä¹‰JSONç¼–ç å™¨å¤„ç†NumPyç±»å‹
        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return super().default(obj)
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  åŸºç¡€æ¨¡å‹å“åº”: {base_file}")
        print(f"  RLHFæ¨¡å‹å“åº”: {rlhf_file}")
        print(f"  åˆ†æç»“æœ: {analysis_file}")
    
    def print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°å®éªŒç»“æœæ‘˜è¦"""
        overall = analysis["overall_metrics"]
        conclusion = analysis["rq2_conclusion"]
        
        print("\n" + "="*60)
        print("RQ2å®éªŒç»“æœæ‘˜è¦")
        print("="*60)
        
        print(f"\nğŸ“Š æ•´ä½“æ‹’ç­”ç‡:")
        print(f"  åŸºç¡€æ¨¡å‹: {overall['base_model']['refusal_rate']:.3f}")
        print(f"  RLHFæ¨¡å‹: {overall['rlhf_model']['refusal_rate']:.3f}")
        print(f"  å·®å¼‚: {overall['refusal_rate_increase']:+.3f}")
        
        print(f"\nğŸ“ˆ å¹³å‡æ‹’ç­”ç½®ä¿¡åº¦:")
        print(f"  åŸºç¡€æ¨¡å‹: {overall['base_model']['avg_refusal_confidence']:.3f}")
        print(f"  RLHFæ¨¡å‹: {overall['rlhf_model']['avg_refusal_confidence']:.3f}")
        print(f"  å·®å¼‚: {overall['confidence_increase']:+.3f}")
        
        print(f"\nğŸ” RQ2ç»“è®º:")
        if conclusion['over_refusal_detected']:
            print(f"  âœ… æ£€æµ‹åˆ°RLHFè¿‡åº¦æ‹’ç­”ç°è±¡")
            print(f"  ğŸ“ˆ æ‹’ç­”ç‡å¢åŠ : {conclusion['over_refusal_magnitude']:.3f}")
        else:
            print(f"  âŒ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„è¿‡åº¦æ‹’ç­”ç°è±¡")
        
        if conclusion['statistical_significance']['significant_at_0.05']:
            print(f"  ğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§: p < 0.05 (p = {conclusion['statistical_significance']['p_value']:.4f})")
        else:
            print(f"  ğŸ“Š ç»Ÿè®¡ä¸æ˜¾è‘—: p = {conclusion['statistical_significance']['p_value']:.4f}")
        
        print("\nğŸ“‹ æŒ‰é—®é¢˜ç±»å‹åˆ†æ:")
        for q_type, data in analysis["by_question_type"].items():
            print(f"  {q_type}:")
            print(f"    åŸºç¡€æ¨¡å‹æ‹’ç­”ç‡: {data['base_refusal_rate']:.3f}")
            print(f"    RLHFæ¨¡å‹æ‹’ç­”ç‡: {data['rlhf_refusal_rate']:.3f}")
            print(f"    æ ·æœ¬æ•°é‡: {data['count']}")


def run_rq2_experiment(config: RQ2ExperimentConfig = None):
    """è¿è¡ŒRQ2å®éªŒ"""
    if config is None:
        config = RQ2ExperimentConfig()
    
    experimenter = RQ2Experimenter(config)
    analysis = experimenter.compare_models()
    experimenter.print_summary(analysis)
    
    return analysis


if __name__ == "__main__":
    # éœ€è¦å®‰è£…scipy for statistical tests
    try:
        import pandas as pd
        from scipy import stats
    except ImportError:
        print("è¯·å®‰è£…ä¾èµ–: pip install pandas scipy")
        exit(1)
    
    # è¿è¡Œå®éªŒ
    config = RQ2ExperimentConfig()
    analysis = run_rq2_experiment(config)
