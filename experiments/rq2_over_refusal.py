"""
RQ2å®éªŒæ ¸å¿ƒå¼•æ“: RLHFè¿‡åº¦æ‹’ç­”ç°è±¡åˆ†æ
================================================

ç ”ç©¶é—®é¢˜ (RQ2):
    RLHFæ˜¯å¦åœ¨å¯å›ç­”çš„è®°å¿†æ£€ç´¢åœºæ™¯ä¸­è¿‡äºä¿å®ˆï¼Œå¯¼è‡´é”™è¯¯æ‹’ç­”ï¼Ÿ

åŠŸèƒ½æ¦‚è¿°:
    æœ¬æ–‡ä»¶æ˜¯RQ2å®éªŒçš„æ ¸å¿ƒå®ç°ï¼ŒåŒ…å«å®Œæ•´çš„å®éªŒé€»è¾‘ã€æ¨¡å‹å¯¹æ¯”ã€
    æ‹’ç­”æ£€æµ‹å’Œç»“æœåˆ†æåŠŸèƒ½ã€‚è´Ÿè´£æ‰§è¡ŒåŸºç¡€æ¨¡å‹ä¸RLHFæ¨¡å‹çš„æ‹’ç­”è¡Œä¸ºå¯¹æ¯”å®éªŒã€‚

ä¸»è¦ç»„ä»¶:

1. **æ•°æ®ç»“æ„å®šä¹‰**
   - RQ2ExperimentConfig: å®éªŒå‚æ•°é…ç½®ç±»
   - ModelResponse: æ¨¡å‹å“åº”ç»“æœå°è£…ç±»

2. **RefusalDetector æ‹’ç­”æ£€æµ‹å™¨** [æ ¸å¿ƒç®—æ³•]
   - detect_refusal(): ä¸»æ£€æµ‹æ¥å£ï¼Œæ”¯æŒRoBERTaå’Œè§„åˆ™ä¸¤ç§æ–¹æ³•
   - _detect_with_roberta(): åŸºäºRoBERTa-base-squad2çš„ä¸“ä¸šæ£€æµ‹
   - _detect_with_rules(): æ”¹è¿›çš„è§„åˆ™æ£€æµ‹ç®—æ³• (100%æµ‹è¯•å‡†ç¡®ç‡)
   - get_detection_method(): è¿”å›å½“å‰ä½¿ç”¨çš„æ£€æµ‹æ–¹æ³•

3. **RQ2Experimenter å®éªŒæ‰§è¡Œå™¨** [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘]
   - load_model(): åŠ è½½Hugging Faceæ¨¡å‹ (åŸºç¡€æ¨¡å‹/RLHFæ¨¡å‹)
   - create_prompt(): ç”Ÿæˆæ ‡å‡†åŒ–çš„å¯¹è¯å†å²æç¤º
   - generate_response(): æ‰§è¡Œæ¨¡å‹æ¨ç†ç”Ÿæˆå“åº”
   - evaluate_model(): è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ‹’ç­”è¡Œä¸º
   - compare_models(): å¯¹æ¯”åŸºç¡€æ¨¡å‹vs RLHFæ¨¡å‹çš„æ‹’ç­”å·®å¼‚
   - analyze_responses(): ç»Ÿè®¡åˆ†ææ‹’ç­”ç‡ã€ç½®ä¿¡åº¦ã€é—®é¢˜ç±»å‹åˆ†å¸ƒ
   - calculate_significance(): è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§ (å¡æ–¹æ£€éªŒ)
   - save_results(): ä¿å­˜å®éªŒç»“æœåˆ°JSONæ–‡ä»¶
   - print_summary(): è¾“å‡ºå®éªŒç»“æœæ‘˜è¦æŠ¥å‘Š

4. **run_rq2_experiment() ä¸»å®éªŒå‡½æ•°**
   - å®éªŒå…¥å£ç‚¹ï¼Œå¯è¢«å¤–éƒ¨è„šæœ¬è°ƒç”¨
   - è‡ªåŠ¨åˆ›å»ºå®éªŒå™¨å¹¶æ‰§è¡Œå®Œæ•´æµç¨‹

å®éªŒæµç¨‹:
    1. æ•°æ®åŠ è½½ â†’ LongMemEvalæ•°æ®é›†ç­›é€‰RQ2ç›¸å…³å®ä¾‹ (æœ‰è¯æ®ä¸”å¯å›ç­”)
    2. æ¨¡å‹åŠ è½½ â†’ åˆ†åˆ«åŠ è½½åŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹
    3. æ¨ç†ç”Ÿæˆ â†’ å¯¹æ¯ä¸ªå®ä¾‹ç”Ÿæˆæ¨¡å‹å“åº”
    4. æ‹’ç­”æ£€æµ‹ â†’ ä½¿ç”¨ä¼˜åŒ–ç®—æ³•æ£€æµ‹å“åº”æ˜¯å¦ä¸ºæ‹’ç­”
    5. å¯¹æ¯”åˆ†æ â†’ è®¡ç®—æ‹’ç­”ç‡å·®å¼‚å’Œç»Ÿè®¡æ˜¾è‘—æ€§
    6. ç»“æœä¿å­˜ â†’ ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’ŒåŸå§‹æ•°æ®

æ”¯æŒçš„æ¨¡å‹:
    - meta-llama/Llama-2-7b-hf (åŸºç¡€)
    - meta-llama/Llama-2-7b-chat-hf (RLHF)
    - meta-llama/Llama-2-13b-hf/chat-hf
    - mistralai/Mistral-7B-v0.1/Instruct-v0.2
    - å…¶ä»–Hugging Faceå…¼å®¹æ¨¡å‹

è¾“å‡ºç»“æœ:
    - æ‹’ç­”ç‡å¯¹æ¯” (åŸºç¡€ vs RLHF)
    - æŒ‰é—®é¢˜ç±»å‹çš„åˆ†æ
    - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    - åŸå§‹å“åº”æ•°æ® (JSONæ ¼å¼)
    - å®éªŒé…ç½®è®°å½•

ä½¿ç”¨ç¤ºä¾‹:
    ```python
    from experiments.rq2_over_refusal import RQ2ExperimentConfig, run_rq2_experiment
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = RQ2ExperimentConfig(
        base_model_name="meta-llama/Llama-2-7b-hf",
        rlhf_model_name="meta-llama/Llama-2-7b-chat-hf",
        output_dir="results/my_experiment"
    )
    
    # è¿è¡Œå®éªŒ
    results = run_rq2_experiment(config)
    ```

ä¾èµ–è¦æ±‚:
    - PyTorch >= 2.0.0
    - Transformers >= 4.30.0
    - LongMemEvalæ•°æ®é›†
    - GPUæ¨è (CPUä¹Ÿå¯è¿è¡Œä½†è¾ƒæ…¢)

æ³¨æ„äº‹é¡¹:
    - å¤§æ¨¡å‹æ¨ç†éœ€è¦è¾ƒå¤šGPUæ˜¾å­˜
    - å®Œæ•´å®éªŒå¯èƒ½éœ€è¦æ•°å°æ—¶è¿è¡Œæ—¶é—´
    - ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ä¾èµ–äºè¶³å¤Ÿçš„æ ·æœ¬æ•°é‡
"""
import json
import os
import torch
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm

from data.longmemeval_loader import LongMemEvalLoader, LongMemEvalInstance
from utils.refusal_detector import RefusalDetector


@dataclass
class RQ2ExperimentConfig:
    """RQ2å®éªŒé…ç½®"""
    # æ¨¡å‹é…ç½®
    base_model_name: str = "Qwen/Qwen2.5-3B"  # åŸºç¡€æ¨¡å‹
    rlhf_model_name: str = "Qwen/Qwen2.5-3B-Instruct"  # RLHFæ¨¡å‹
    
    # æ•°æ®é…ç½®
    longmemeval_path: str = "/mnt/d/datasets/longmemeval_data/longmemeval_oracle.json"
    max_sessions: Optional[int] = None  # æœ€å¤§ä¼šè¯æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    max_instances: Optional[int] = None  # æœ€å¤§å®ä¾‹æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    max_tokens: int = 8192  # æœ€å¤§ç”Ÿæˆtokenæ•°
    
    # å®éªŒå‚æ•°
    temperature: float = 0.1
    top_p: float = 0.9
    max_new_tokens: int = 200
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "results/rq2_qwen2.5_3b"
    save_responses: bool = True


@dataclass
class ModelResponse:
    """æ¨¡å‹å“åº”ç»“æœ"""
    question_id: str
    model_name: str
    question: str
    context_length: int
    response: str
    is_refusal: bool
    refusal_confidence: float
    has_evidence: bool
    answer_quality: Optional[float] = None



class RQ2Experimenter:
    """RQ2å®éªŒæ‰§è¡Œå™¨"""
    
    def __init__(self, config: RQ2ExperimentConfig):
        self.config = config
        self.refusal_detector = RefusalDetector()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ•°æ®
        self.loader = LongMemEvalLoader(config.longmemeval_path)
        self.rq2_instances = self.loader.get_rq2_instances()
        
        # åº”ç”¨å®ä¾‹æ•°é‡é™åˆ¶
        if config.max_instances is not None:
            self.rq2_instances = self.rq2_instances[:config.max_instances]
            print(f"âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶ä¸ºå‰ {config.max_instances} ä¸ªå®ä¾‹")
        
        print(f"åŠ è½½äº† {len(self.rq2_instances)} ä¸ªRQ2å®éªŒå®ä¾‹")
    
    def load_model(self, model_name: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç®€åŒ–çš„æ¨¡å‹åŠ è½½é…ç½®
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",  # è®©transformersè‡ªåŠ¨é€‰æ‹©
            trust_remote_code=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        return model, tokenizer
    
    def create_prompt(self, instance: LongMemEvalInstance) -> str:
        """åˆ›å»ºæ¨¡å‹è¾“å…¥æç¤º"""
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        history = self.loader.format_conversation_history(
            instance, max_sessions=self.config.max_sessions
        )
        
        prompt = f"""You are a helpful AI assistant. Below is a conversation history between a user and an assistant, followed by a question. Please answer the question based on the information provided in the conversation history.

Conversation History:
{history}

Question: {instance.question}

Answer:"""
        
        return prompt
    
    def generate_response(self, model: AutoModelForCausalLM, 
                         tokenizer: AutoTokenizer, 
                         prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=self.config.max_tokens
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def evaluate_model(self, model_name: str) -> List[ModelResponse]:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\nå¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
        
        model, tokenizer = self.load_model(model_name)
        responses = []
        
        for instance in tqdm(self.rq2_instances, desc=f"è¯„ä¼° {model_name}"):
            # åˆ›å»ºæç¤º
            prompt = self.create_prompt(instance)
            
            # ç”Ÿæˆå“åº”
            response_text = self.generate_response(model, tokenizer, prompt)
            
            # æ£€æµ‹æ‹’ç­” - ä½¿ç”¨è§„åˆ™æ£€æµ‹æ–¹æ³•
            is_refusal, refusal_confidence = self.refusal_detector.detect_refusal(
                response=response_text,
                question=instance.question
            )
            
            # åˆ›å»ºå“åº”å¯¹è±¡
            response = ModelResponse(
                question_id=instance.question_id,
                model_name=model_name,
                question=instance.question,
                context_length=len(prompt),
                response=response_text,
                is_refusal=is_refusal,
                refusal_confidence=refusal_confidence,
                has_evidence=instance.has_evidence_in_context
            )
            
            responses.append(response)
        
        # æ¸…ç†GPUå†…å­˜
        del model
        torch.cuda.empty_cache()
        
        return responses
    
    def compare_models(self) -> Dict[str, Any]:
        """æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹"""
        print("å¼€å§‹RQ2å®éªŒ: æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹çš„æ‹’ç­”è¡Œä¸º")
        
        # è¯„ä¼°åŸºç¡€æ¨¡å‹
        base_responses = self.evaluate_model(self.config.base_model_name)
        
        # è¯„ä¼°RLHFæ¨¡å‹
        rlhf_responses = self.evaluate_model(self.config.rlhf_model_name)
        
        # åˆ†æç»“æœ
        analysis = self.analyze_responses(base_responses, rlhf_responses)
        
        # ä¿å­˜ç»“æœ
        if self.config.save_responses:
            self.save_results(base_responses, rlhf_responses, analysis)
        
        return analysis
    
    def analyze_responses(self, base_responses: List[ModelResponse], 
                         rlhf_responses: List[ModelResponse]) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹å“åº”å·®å¼‚"""
        
        # è®¡ç®—æ‹’ç­”ç‡
        base_refusal_rate = sum(r.is_refusal for r in base_responses) / len(base_responses)
        rlhf_refusal_rate = sum(r.is_refusal for r in rlhf_responses) / len(rlhf_responses)
        
        # è®¡ç®—å¹³å‡æ‹’ç­”ç½®ä¿¡åº¦
        base_refusal_conf = np.mean([r.refusal_confidence for r in base_responses])
        rlhf_refusal_conf = np.mean([r.refusal_confidence for r in rlhf_responses])
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
        question_types = {}
        for response in base_responses:
            q_type = next(
                (inst.question_type for inst in self.rq2_instances 
                 if inst.question_id == response.question_id), 
                "unknown"
            )
            if q_type not in question_types:
                question_types[q_type] = {"base": [], "rlhf": []}
            question_types[q_type]["base"].append(response.is_refusal)
        
        for response in rlhf_responses:
            q_type = next(
                (inst.question_type for inst in self.rq2_instances 
                 if inst.question_id == response.question_id), 
                "unknown"
            )
            question_types[q_type]["rlhf"].append(response.is_refusal)
        
        # è®¡ç®—å„ç±»å‹çš„æ‹’ç­”ç‡
        type_analysis = {}
        for q_type, data in question_types.items():
            type_analysis[q_type] = {
                "base_refusal_rate": np.mean(data["base"]) if data["base"] else 0,
                "rlhf_refusal_rate": np.mean(data["rlhf"]) if data["rlhf"] else 0,
                "count": len(data["base"])
            }
        
        analysis = {
            "overall_metrics": {
                "base_model": {
                    "refusal_rate": base_refusal_rate,
                    "avg_refusal_confidence": base_refusal_conf,
                    "total_instances": len(base_responses)
                },
                "rlhf_model": {
                    "refusal_rate": rlhf_refusal_rate,
                    "avg_refusal_confidence": rlhf_refusal_conf,
                    "total_instances": len(rlhf_responses)
                },
                "refusal_rate_increase": rlhf_refusal_rate - base_refusal_rate,
                "confidence_increase": rlhf_refusal_conf - base_refusal_conf
            },
            "by_question_type": type_analysis,
            "rq2_conclusion": {
                "over_refusal_detected": rlhf_refusal_rate > base_refusal_rate,
                "over_refusal_magnitude": rlhf_refusal_rate - base_refusal_rate,
                "statistical_significance": self.calculate_significance(
                    base_responses, rlhf_responses
                )
            }
        }
        
        return analysis
    
    def calculate_significance(self, base_responses: List[ModelResponse], 
                             rlhf_responses: List[ModelResponse]) -> Dict[str, float]:
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
        from scipy import stats
        
        base_refusals = [r.is_refusal for r in base_responses]
        rlhf_refusals = [r.is_refusal for r in rlhf_responses]
        
        # ä½¿ç”¨å¡æ–¹æ£€éªŒ
        contingency_table = [
            [sum(base_refusals), len(base_refusals) - sum(base_refusals)],
            [sum(rlhf_refusals), len(rlhf_refusals) - sum(rlhf_refusals)]
        ]
        
        chi2, p_value = stats.chi2_contingency(contingency_table)[:2]
        
        return {
            "chi2_statistic": float(chi2),
            "p_value": float(p_value),
            "significant_at_0.05": p_value < 0.05
        }
    
    def save_results(self, base_responses: List[ModelResponse], 
                    rlhf_responses: List[ModelResponse], 
                    analysis: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åŸå§‹å“åº”
        base_file = self.output_dir / f"rq2_base_responses_{timestamp}.json"
        rlhf_file = self.output_dir / f"rq2_rlhf_responses_{timestamp}.json"
        analysis_file = self.output_dir / f"rq2_analysis_{timestamp}.json"
        
        with open(base_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in base_responses], f, indent=2, ensure_ascii=False)
        
        with open(rlhf_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in rlhf_responses], f, indent=2, ensure_ascii=False)
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  åŸºç¡€æ¨¡å‹å“åº”: {base_file}")
        print(f"  RLHFæ¨¡å‹å“åº”: {rlhf_file}")
        print(f"  åˆ†æç»“æœ: {analysis_file}")
    
    def print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°å®éªŒç»“æœæ‘˜è¦"""
        overall = analysis["overall_metrics"]
        conclusion = analysis["rq2_conclusion"]
        
        print("\n" + "="*60)
        print("RQ2å®éªŒç»“æœæ‘˜è¦")
        print("="*60)
        
        print(f"\nğŸ“Š æ•´ä½“æ‹’ç­”ç‡:")
        print(f"  åŸºç¡€æ¨¡å‹: {overall['base_model']['refusal_rate']:.3f}")
        print(f"  RLHFæ¨¡å‹: {overall['rlhf_model']['refusal_rate']:.3f}")
        print(f"  å·®å¼‚: {overall['refusal_rate_increase']:+.3f}")
        
        print(f"\nğŸ“ˆ å¹³å‡æ‹’ç­”ç½®ä¿¡åº¦:")
        print(f"  åŸºç¡€æ¨¡å‹: {overall['base_model']['avg_refusal_confidence']:.3f}")
        print(f"  RLHFæ¨¡å‹: {overall['rlhf_model']['avg_refusal_confidence']:.3f}")
        print(f"  å·®å¼‚: {overall['confidence_increase']:+.3f}")
        
        print(f"\nğŸ” RQ2ç»“è®º:")
        if conclusion['over_refusal_detected']:
            print(f"  âœ… æ£€æµ‹åˆ°RLHFè¿‡åº¦æ‹’ç­”ç°è±¡")
            print(f"  ğŸ“ˆ æ‹’ç­”ç‡å¢åŠ : {conclusion['over_refusal_magnitude']:.3f}")
        else:
            print(f"  âŒ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„è¿‡åº¦æ‹’ç­”ç°è±¡")
        
        if conclusion['statistical_significance']['significant_at_0.05']:
            print(f"  ğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§: p < 0.05 (p = {conclusion['statistical_significance']['p_value']:.4f})")
        else:
            print(f"  ğŸ“Š ç»Ÿè®¡ä¸æ˜¾è‘—: p = {conclusion['statistical_significance']['p_value']:.4f}")
        
        print("\nğŸ“‹ æŒ‰é—®é¢˜ç±»å‹åˆ†æ:")
        for q_type, data in analysis["by_question_type"].items():
            print(f"  {q_type}:")
            print(f"    åŸºç¡€æ¨¡å‹æ‹’ç­”ç‡: {data['base_refusal_rate']:.3f}")
            print(f"    RLHFæ¨¡å‹æ‹’ç­”ç‡: {data['rlhf_refusal_rate']:.3f}")
            print(f"    æ ·æœ¬æ•°é‡: {data['count']}")


def run_rq2_experiment(config: RQ2ExperimentConfig = None):
    """è¿è¡ŒRQ2å®éªŒ"""
    if config is None:
        config = RQ2ExperimentConfig()
    
    experimenter = RQ2Experimenter(config)
    analysis = experimenter.compare_models()
    experimenter.print_summary(analysis)
    
    return analysis


if __name__ == "__main__":
    # éœ€è¦å®‰è£…scipy for statistical tests
    try:
        import pandas as pd
        from scipy import stats
    except ImportError:
        print("è¯·å®‰è£…ä¾èµ–: pip install pandas scipy")
        exit(1)
    
    # è¿è¡Œå®éªŒ
    config = RQ2ExperimentConfig()
    analysis = run_rq2_experiment(config)
