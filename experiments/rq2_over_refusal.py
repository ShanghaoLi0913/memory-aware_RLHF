"""
RQ2å®éªŒæ ¸å¿ƒå¼•æ“: RLHFè¿‡åº¦æ‹’ç­”ç°è±¡åˆ†æ
================================================

ç ”ç©¶é—®é¢˜ (RQ2):
    RLHFæ˜¯å¦åœ¨å¯å›ç­”çš„è®°å¿†æ£€ç´¢åœºæ™¯ä¸­è¿‡äºä¿å®ˆï¼Œå¯¼è‡´é”™è¯¯æ‹’ç­”ï¼Ÿ

åŠŸèƒ½æ¦‚è¿°:
    æœ¬æ–‡ä»¶æ˜¯RQ2å®éªŒçš„æ ¸å¿ƒå®ç°ï¼ŒåŒ…å«å®Œæ•´çš„å®éªŒé€»è¾‘ã€æ¨¡å‹å¯¹æ¯”ã€
    æ‹’ç­”æ£€æµ‹å’Œç»“æœåˆ†æåŠŸèƒ½ã€‚è´Ÿè´£æ‰§è¡ŒåŸºç¡€æ¨¡å‹ä¸RLHFæ¨¡å‹çš„æ‹’ç­”è¡Œä¸ºå¯¹æ¯”å®éªŒã€‚

ä¸»è¦ç»„ä»¶:

1. **æ•°æ®ç»“æ„å®šä¹‰**
   - RQ2ExperimentConfig: å®éªŒå‚æ•°é…ç½®ç±»
   - ModelResponse: æ¨¡å‹å“åº”ç»“æœå°è£…ç±»

2. **RefusalDetector æ‹’ç­”æ£€æµ‹å™¨** [æ ¸å¿ƒç®—æ³•]
   - detect_refusal(): ä¸»æ£€æµ‹æ¥å£ï¼Œæ”¯æŒRoBERTaå’Œè§„åˆ™ä¸¤ç§æ–¹æ³•
   - _detect_with_roberta(): åŸºäºRoBERTa-base-squad2çš„ä¸“ä¸šæ£€æµ‹
   - _detect_with_rules(): æ”¹è¿›çš„è§„åˆ™æ£€æµ‹ç®—æ³• (100%æµ‹è¯•å‡†ç¡®ç‡)
   - get_detection_method(): è¿”å›å½“å‰ä½¿ç”¨çš„æ£€æµ‹æ–¹æ³•

3. **RQ2Experimenter å®éªŒæ‰§è¡Œå™¨** [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘]
   - load_model(): åŠ è½½Hugging Faceæ¨¡å‹ (åŸºç¡€æ¨¡å‹/RLHFæ¨¡å‹)
   - create_prompt(): ç”Ÿæˆæ ‡å‡†åŒ–çš„å¯¹è¯å†å²æç¤º
   - generate_response(): æ‰§è¡Œæ¨¡å‹æ¨ç†ç”Ÿæˆå“åº”
   - evaluate_model(): è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ‹’ç­”è¡Œä¸º
   - compare_models(): å¯¹æ¯”åŸºç¡€æ¨¡å‹vs RLHFæ¨¡å‹çš„æ‹’ç­”å·®å¼‚
   - analyze_responses(): ç»Ÿè®¡åˆ†ææ‹’ç­”ç‡ã€ç½®ä¿¡åº¦ã€é—®é¢˜ç±»å‹åˆ†å¸ƒ
   - calculate_significance(): è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§ (å¡æ–¹æ£€éªŒ)
   - save_results(): ä¿å­˜å®éªŒç»“æœåˆ°JSONæ–‡ä»¶
   - print_summary(): è¾“å‡ºå®éªŒç»“æœæ‘˜è¦æŠ¥å‘Š

4. **run_rq2_experiment() ä¸»å®éªŒå‡½æ•°**
   - å®éªŒå…¥å£ç‚¹ï¼Œå¯è¢«å¤–éƒ¨è„šæœ¬è°ƒç”¨
   - è‡ªåŠ¨åˆ›å»ºå®éªŒå™¨å¹¶æ‰§è¡Œå®Œæ•´æµç¨‹

å®éªŒæµç¨‹:
    1. æ•°æ®åŠ è½½ â†’ LongMemEvalæ•°æ®é›†ç­›é€‰RQ2ç›¸å…³å®ä¾‹ (æœ‰è¯æ®ä¸”å¯å›ç­”)
    2. æ¨¡å‹åŠ è½½ â†’ åˆ†åˆ«åŠ è½½åŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹
    3. æ¨ç†ç”Ÿæˆ â†’ å¯¹æ¯ä¸ªå®ä¾‹ç”Ÿæˆæ¨¡å‹å“åº”
    4. æ‹’ç­”æ£€æµ‹ â†’ ä½¿ç”¨ä¼˜åŒ–ç®—æ³•æ£€æµ‹å“åº”æ˜¯å¦ä¸ºæ‹’ç­”
    5. å¯¹æ¯”åˆ†æ â†’ è®¡ç®—æ‹’ç­”ç‡å·®å¼‚å’Œç»Ÿè®¡æ˜¾è‘—æ€§
    6. ç»“æœä¿å­˜ â†’ ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’ŒåŸå§‹æ•°æ®

æ”¯æŒçš„æ¨¡å‹:
    - meta-llama/Llama-2-7b-hf (åŸºç¡€)
    - meta-llama/Llama-2-7b-chat-hf (RLHF)
    - meta-llama/Llama-2-13b-hf/chat-hf
    - mistralai/Mistral-7B-v0.1/Instruct-v0.2
    - å…¶ä»–Hugging Faceå…¼å®¹æ¨¡å‹

è¾“å‡ºç»“æœ:
    - æ‹’ç­”ç‡å¯¹æ¯” (åŸºç¡€ vs RLHF)
    - æŒ‰é—®é¢˜ç±»å‹çš„åˆ†æ
    - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    - åŸå§‹å“åº”æ•°æ® (JSONæ ¼å¼)
    - å®éªŒé…ç½®è®°å½•

ä½¿ç”¨ç¤ºä¾‹:
    ```python
    from experiments.rq2_over_refusal import RQ2ExperimentConfig, run_rq2_experiment
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = RQ2ExperimentConfig(
        base_model_name="meta-llama/Llama-2-7b-hf",
        rlhf_model_name="meta-llama/Llama-2-7b-chat-hf",
        output_dir="results/my_experiment"
    )
    
    # è¿è¡Œå®éªŒ
    results = run_rq2_experiment(config)
    ```

ä¾èµ–è¦æ±‚:
    - PyTorch >= 2.0.0
    - Transformers >= 4.30.0
    - LongMemEvalæ•°æ®é›†
    - GPUæ¨è (CPUä¹Ÿå¯è¿è¡Œä½†è¾ƒæ…¢)

æ³¨æ„äº‹é¡¹:
    - å¤§æ¨¡å‹æ¨ç†éœ€è¦è¾ƒå¤šGPUæ˜¾å­˜
    - å®Œæ•´å®éªŒå¯èƒ½éœ€è¦æ•°å°æ—¶è¿è¡Œæ—¶é—´
    - ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ä¾èµ–äºè¶³å¤Ÿçš„æ ·æœ¬æ•°é‡
"""
import json
import os
import torch
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

# åœ¨å¯¼å…¥transformersä¹‹å‰è®¾ç½®HFé•œåƒæº
if not os.environ.get('HF_ENDPOINT'):
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    print(f"ğŸŒ è‡ªåŠ¨è®¾ç½®HFé•œåƒæº: {os.environ['HF_ENDPOINT']}")

import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm

from data.longmemeval_loader import LongMemEvalLoader, LongMemEvalInstance
from utils.refusal_detector import RefusalDetector
try:
    # ä½¿ç”¨æ•°æ®é›†ä½œè€…æä¾›çš„è¯„ä¼°å®ç°
    from evaluate_qa import (
        get_anscheck_prompt,
        model_zoo,
        chat_completions_with_backoff,
        OpenAI,
    )
    import openai
    OPENAI_AVAILABLE = True
except Exception:
    # ä»ç„¶å…è®¸æ²¡æœ‰OpenAIä¾èµ–æ—¶è¿è¡Œï¼ˆå°†å›é€€åˆ°EM/F1ï¼‰
    OPENAI_AVAILABLE = False
    print("âš ï¸ æœªèƒ½å¯¼å…¥evaluate_qa.py/OpenAIï¼ŒIE-Accå°†å›é€€åˆ°EM/F1åŒ¹é…")


# -------------------------------
# æ–‡æœ¬åŒ¹é…è¯„ä¼° (EM / F1)
# -------------------------------
import re
import string


def _normalize_text(text: str) -> str:
    if text is None:
        return ""
    text = text.lower()
    # ç§»é™¤æ ‡ç‚¹
    text = text.translate(str.maketrans('', '', string.punctuation))
    # ç§»é™¤å† è¯
    text = re.sub(r"\b(a|an|the)\b", " ", text)
    # åˆå¹¶ç©ºç™½
    text = re.sub(r"\s+", " ", text).strip()
    return text


def exact_match(prediction: str, ground_truth: str) -> int:
    return int(_normalize_text(prediction) == _normalize_text(ground_truth))


def f1_score(prediction: str, ground_truth: str) -> float:
    pred_tokens = _normalize_text(prediction).split()
    truth_tokens = _normalize_text(ground_truth).split()
    if len(pred_tokens) == 0 and len(truth_tokens) == 0:
        return 1.0
    if len(pred_tokens) == 0 or len(truth_tokens) == 0:
        return 0.0
    common = {}
    for t in pred_tokens:
        common[t] = min(pred_tokens.count(t), truth_tokens.count(t))
    num_same = sum(common.values())
    if num_same == 0:
        return 0.0
    precision = num_same / len(pred_tokens)
    recall = num_same / len(truth_tokens)
    return 2 * precision * recall / (precision + recall)


@dataclass
class RQ2ExperimentConfig:
    """RQ2å®éªŒé…ç½®"""
    # æ¨¡å‹é…ç½®
    base_model_name: str = "Qwen/Qwen2.5-3B"  # åŸºç¡€æ¨¡å‹
    rlhf_model_name: str = "Qwen/Qwen2.5-3B-Instruct"  # RLHFæ¨¡å‹
    
    # æ•°æ®é…ç½®
    longmemeval_path: str = "data/longmemeval_data/longmemeval_oracle.json"
    max_sessions: Optional[int] = None  # æœ€å¤§ä¼šè¯æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    max_instances: Optional[int] = None  # æœ€å¤§å®ä¾‹æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    max_tokens: int = 28000   # RTX 4070ä¸´æ—¶è®¾ç½®ï¼Œåç»­äº‘ç«¯ç”¨28000
    
    # å®éªŒå‚æ•°
    temperature: float = 0.1
    top_p: float = 0.9
    max_new_tokens: int = 100  # å‡å°‘è¾“å‡ºé•¿åº¦ï¼Œæ‹’ç­”æ£€æµ‹ä¸éœ€è¦å¤ªé•¿å“åº”
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "results/rq2_qwen2.5_3b"
    save_responses: bool = True


@dataclass
class ModelResponse:
    """æ¨¡å‹å“åº”ç»“æœ"""
    question_id: str
    model_name: str
    question: str
    context_length: int
    response: str
    is_refusal: bool
    refusal_confidence: float
    has_evidence: bool
    answer_quality: Optional[float] = None
    is_correct: Optional[bool] = None  # QAå‡†ç¡®æ€§è¯„ä¼°ç»“æœ
    ground_truth_answer: Optional[str] = None  # æ ‡å‡†ç­”æ¡ˆ


class QAEvaluator:
    """QAå‡†ç¡®æ€§è¯„ä¼°å™¨
    ä¼˜å…ˆä½¿ç”¨LongMemEvalä½œè€…æä¾›çš„ evaluate_qa.py (LLMåˆ¤å¯¹)ï¼Œ
    è‹¥ä¸å¯ç”¨åˆ™å›é€€åˆ°æœ¬åœ° EM/F1 åŒ¹é…ã€‚
    """
    
    def __init__(self, metric_model: str = "gpt-4o-mini"):
        """
        åˆå§‹åŒ–QAè¯„ä¼°å™¨
        
        Args:
            metric_model: ç”¨äºè¯„ä¼°çš„æ¨¡å‹åç§°
            use_openai: æ˜¯å¦ä½¿ç”¨OpenAI API
        """
        self.metric_model = metric_model
        self.openai_available = OPENAI_AVAILABLE

        if self.openai_available:
            try:
                import os
                self.client = OpenAI(
                    api_key=os.getenv('OPENAI_API_KEY', ''),
                    organization=os.getenv('OPENAI_ORGANIZATION', None)
                )
                print(f"âœ… IE-Accè¯„ä¼°å°†ä½¿ç”¨ evaluate_qa.py (æ¨¡å‹: {metric_model})")
            except Exception as e:
                print(f"âš ï¸ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œå°†å›é€€åˆ°EM/F1: {e}")
                self.openai_available = False
        else:
            print("ğŸ“ æœªæ£€æµ‹åˆ°evaluate_qaä¾èµ–/å¯†é’¥ï¼Œå°†å›é€€åˆ°EM/F1åŒ¹é…")
    
    def get_anscheck_prompt(self, task_type: str, question: str, answer: str, response: str, is_abstention: bool = False) -> str:
        # ç›´æ¥å¤ç”¨ä½œè€…è„šæœ¬çš„æ¨¡æ¿ç”Ÿæˆé€»è¾‘
        return get_anscheck_prompt(task_type, question, answer, response, abstention=is_abstention)
    
    def evaluate_response(self, instance: LongMemEvalInstance, response: str) -> bool:
        """
        è¯„ä¼°å•ä¸ªå“åº”çš„å‡†ç¡®æ€§
        
        Args:
            instance: LongMemEvalæ•°æ®å®ä¾‹
            response: æ¨¡å‹å“åº”
            
        Returns:
            bool: æ˜¯å¦æ­£ç¡®
        """
        if not self.openai_available:
            return None
            
        try:
            is_abstention = instance.is_abstention
            prompt = self.get_anscheck_prompt(
                task_type=instance.question_type,
                question=instance.question,
                answer=instance.answer,
                response=response,
                is_abstention=is_abstention
            )
            
            kwargs = {
                'model': self.metric_model,
                'messages': [{"role": "user", "content": prompt}],
                'n': 1,
                'temperature': 0,
                'max_tokens': 10
            }
            if self.openai_available:
                completion = chat_completions_with_backoff(self.client, **kwargs)
                eval_response = completion.choices[0].message.content.strip()
                return 'yes' in eval_response.lower()
            return None
            
        except Exception as e:
            print(f"âš ï¸ QAè¯„ä¼°å¤±è´¥: {e}")
            return None


class RQ2Experimenter:
    """RQ2å®éªŒæ‰§è¡Œå™¨"""
    
    def __init__(self, config: RQ2ExperimentConfig):
        import os
        self.config = config
        self.refusal_detector = RefusalDetector()
        
        # è‡ªåŠ¨å¯ç”¨IE-Accè¯„ä¼°ï¼šå½“OpenAIåº“å¯ç”¨ä¸”æ£€æµ‹åˆ°OPENAI_API_KEYæ—¶
        self.enable_qa_eval = bool(os.getenv('OPENAI_API_KEY')) and OPENAI_AVAILABLE
        if self.enable_qa_eval:
            self.qa_evaluator = QAEvaluator()
            print("ğŸ§ª å·²å¯ç”¨IE-Accè¯„ä¼° (æ£€æµ‹åˆ°OPENAI_API_KEY)")
        else:
            self.qa_evaluator = None
            print("ğŸ“ IE-Accè¯„ä¼°æœªå¯ç”¨ï¼ˆæœªæ£€æµ‹åˆ°OPENAI_API_KEYæˆ–OpenAIåº“ä¸å¯ç”¨ï¼‰")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ•°æ®
        self.loader = LongMemEvalLoader(config.longmemeval_path)
        
        # åŠ è½½IEå’ŒABSä¸¤ä¸ªå­é›†
        self.ie_instances = self.loader.get_rq2_instances()  # Information Extraction - åº”è¯¥å›ç­”
        self.abs_instances = self.loader.get_abstention_instances()  # Abstention - åº”è¯¥æ‹’ç­”
        
        # åº”ç”¨å®ä¾‹æ•°é‡é™åˆ¶
        if config.max_instances is not None:
            self.ie_instances = self.ie_instances[:config.max_instances]
            # ABSæ•°é‡è¾ƒå°‘ï¼ŒæŒ‰æ¯”ä¾‹é™åˆ¶
            abs_limit = min(len(self.abs_instances), config.max_instances // 3)
            self.abs_instances = self.abs_instances[:abs_limit]
            print(f"âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šIEé™åˆ¶ä¸ºå‰ {len(self.ie_instances)} ä¸ªï¼ŒABSé™åˆ¶ä¸ºå‰ {len(self.abs_instances)} ä¸ª")
        
        print(f"åŠ è½½äº† {len(self.ie_instances)} ä¸ªIEå®ä¾‹ï¼ˆåº”è¯¥å›ç­”ï¼‰")
        print(f"åŠ è½½äº† {len(self.abs_instances)} ä¸ªABSå®ä¾‹ï¼ˆåº”è¯¥æ‹’ç­”ï¼‰")
    
    def load_model(self, model_name: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        import os
        
        # HFé•œåƒæºå·²åœ¨æ–‡ä»¶å¼€å¤´è®¾ç½®
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç®€åŒ–çš„æ¨¡å‹åŠ è½½é…ç½®
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",  # è®©transformersè‡ªåŠ¨é€‰æ‹©
            trust_remote_code=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        return model, tokenizer
    
    def create_prompt(self, instance: LongMemEvalInstance) -> str:
        """åˆ›å»ºæ¨¡å‹è¾“å…¥æç¤º"""
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        history = self.loader.format_conversation_history(
            instance, max_sessions=self.config.max_sessions
        )
        
        prompt = f"""You are a helpful AI assistant. Below is a conversation history between a user and an assistant, followed by a question. Please answer the question based on the information provided in the conversation history.

Conversation History:
{history}

Question: {instance.question}

Answer:"""
        
        return prompt
    
    def generate_response(self, model: AutoModelForCausalLM, 
                         tokenizer: AutoTokenizer, 
                         prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        inputs = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=self.config.max_tokens
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def evaluate_model(self, model_name: str) -> Tuple[List[ModelResponse], List[ModelResponse]]:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹ï¼Œè¿”å›IEå’ŒABSä¸¤ä¸ªå­é›†çš„å“åº”"""
        print(f"\nå¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
        
        model, tokenizer = self.load_model(model_name)
        
        # è¯„ä¼°IEå®ä¾‹ï¼ˆåº”è¯¥å›ç­”ï¼‰
        ie_responses = []
        for instance in tqdm(self.ie_instances, desc=f"è¯„ä¼° {model_name} - IE"):
            prompt = self.create_prompt(instance)
            response_text = self.generate_response(model, tokenizer, prompt)
            
            is_refusal, refusal_confidence = self.refusal_detector.detect_refusal(
                response=response_text,
                question=instance.question
            )
            
            # QAå‡†ç¡®æ€§è¯„ä¼°ï¼ˆä»…å¯¹éæ‹’ç­”å“åº”è¿›è¡Œï¼‰
            is_correct = None
            if self.enable_qa_eval and not is_refusal and self.qa_evaluator:
                is_correct = self.qa_evaluator.evaluate_response(instance, response_text)
            
            response = ModelResponse(
                question_id=instance.question_id,
                model_name=model_name,
                question=instance.question,
                context_length=len(prompt),
                response=response_text,
                is_refusal=is_refusal,
                refusal_confidence=refusal_confidence,
                has_evidence=instance.has_evidence_in_context,
                is_correct=is_correct,
                ground_truth_answer=instance.answer
            )
            ie_responses.append(response)
        
        # è¯„ä¼°ABSå®ä¾‹ï¼ˆåº”è¯¥æ‹’ç­”ï¼‰
        abs_responses = []
        for instance in tqdm(self.abs_instances, desc=f"è¯„ä¼° {model_name} - ABS"):
            prompt = self.create_prompt(instance)
            response_text = self.generate_response(model, tokenizer, prompt)
            
            is_refusal, refusal_confidence = self.refusal_detector.detect_refusal(
                response=response_text,
                question=instance.question
            )
            
            # QAå‡†ç¡®æ€§è¯„ä¼°ï¼ˆABSåº”è¯¥æ‹’ç­”ï¼Œè¯„ä¼°æ‹’ç­”æ˜¯å¦æ­£ç¡®ï¼‰
            is_correct = None
            if self.enable_qa_eval and self.qa_evaluator:
                is_correct = self.qa_evaluator.evaluate_response(instance, response_text)
            
            response = ModelResponse(
                question_id=instance.question_id,
                model_name=model_name,
                question=instance.question,
                context_length=len(prompt),
                response=response_text,
                is_refusal=is_refusal,
                refusal_confidence=refusal_confidence,
                has_evidence=False,  # ABSå®ä¾‹è®¾è®¡ä¸ºæ— è¯æ®
                is_correct=is_correct,
                ground_truth_answer=instance.answer
            )
            abs_responses.append(response)
        
        # æ¸…ç†GPUå†…å­˜
        del model
        torch.cuda.empty_cache()
        
        return ie_responses, abs_responses
    
    def compare_models(self) -> Dict[str, Any]:
        """æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹åœ¨IEå’ŒABSä¸¤ä¸ªå­é›†ä¸Šçš„è¡¨ç°"""
        print("å¼€å§‹RQ2å®éªŒ: æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒRLHFæ¨¡å‹çš„æ‹’ç­”è¡Œä¸º")
        
        # è¯„ä¼°åŸºç¡€æ¨¡å‹
        base_ie_responses, base_abs_responses = self.evaluate_model(self.config.base_model_name)
        
        # è¯„ä¼°RLHFæ¨¡å‹
        rlhf_ie_responses, rlhf_abs_responses = self.evaluate_model(self.config.rlhf_model_name)
        
        # åˆ†æç»“æœ
        analysis = self.analyze_rq2_responses(
            base_ie_responses, base_abs_responses,
            rlhf_ie_responses, rlhf_abs_responses
        )
        
        # ä¿å­˜ç»“æœ
        if self.config.save_responses:
            self.save_rq2_results(
                base_ie_responses, base_abs_responses,
                rlhf_ie_responses, rlhf_abs_responses,
                analysis
            )
        
        return analysis
    
    def analyze_rq2_responses(self, 
                             base_ie_responses: List[ModelResponse],
                             base_abs_responses: List[ModelResponse],
                             rlhf_ie_responses: List[ModelResponse],
                             rlhf_abs_responses: List[ModelResponse]) -> Dict[str, Any]:
        """åˆ†æRQ2å®éªŒç»“æœï¼Œè®¡ç®—ORRã€IE-Accã€ABS-LegitRefuseç­‰æŒ‡æ ‡"""
        
        # è®¡ç®—IEå­é›†çš„ORR (Over-Refusal Rate)
        base_ie_refusals = sum(1 for r in base_ie_responses if r.is_refusal)
        rlhf_ie_refusals = sum(1 for r in rlhf_ie_responses if r.is_refusal)
        
        base_orr = base_ie_refusals / len(base_ie_responses) if base_ie_responses else 0
        rlhf_orr = rlhf_ie_refusals / len(rlhf_ie_responses) if rlhf_ie_responses else 0
        
        # è®¡ç®—ABSå­é›†çš„åˆæ³•æ‹’ç­”ç‡ (Legitimate Refusal Rate)
        base_abs_refusals = sum(1 for r in base_abs_responses if r.is_refusal)
        rlhf_abs_refusals = sum(1 for r in rlhf_abs_responses if r.is_refusal)
        
        base_abs_legit_refuse = base_abs_refusals / len(base_abs_responses) if base_abs_responses else 0
        rlhf_abs_legit_refuse = rlhf_abs_refusals / len(rlhf_abs_responses) if rlhf_abs_responses else 0
        
        # è®¡ç®—IEå­é›†ä¸­éæ‹’ç­”å›ç­”çš„æ•°é‡å’Œå‡†ç¡®æ€§
        base_ie_non_refusal = [r for r in base_ie_responses if not r.is_refusal]
        rlhf_ie_non_refusal = [r for r in rlhf_ie_responses if not r.is_refusal]
        
        # è®¡ç®—IE-Acc (Information Extraction Accuracy)
        # ä½¿ç”¨evaluate_qa.pyçš„æ–¹æ³•ï¼Œå¦‚æœæ²¡æœ‰QAè¯„ä¼°å™¨åˆ™è®¾ä¸ºNone
        base_ie_correct_count = 0
        rlhf_ie_correct_count = 0
        base_ie_total = len(base_ie_responses)
        rlhf_ie_total = len(rlhf_ie_responses)

        if self.qa_evaluator:
            # ä½¿ç”¨LLMè¯„ä¼°å™¨
            for r in base_ie_responses:
                if not r.is_refusal and r.is_correct is True:
                    base_ie_correct_count += 1
            for r in rlhf_ie_responses:
                if not r.is_refusal and r.is_correct is True:
                    rlhf_ie_correct_count += 1
            
            base_ie_acc = base_ie_correct_count / base_ie_total if base_ie_total else 0
            rlhf_ie_acc = rlhf_ie_correct_count / rlhf_ie_total if rlhf_ie_total else 0
        else:
            # æ²¡æœ‰QAè¯„ä¼°å™¨ï¼Œè®¾ä¸ºNone
            base_ie_acc = None
            rlhf_ie_acc = None
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (McNemar test for paired comparison)
        mcnemar_result = self.calculate_mcnemar_test(base_ie_responses, rlhf_ie_responses)
        
        analysis = {
            'experiment_info': {
                'ie_total_count': len(base_ie_responses),
                'abs_total_count': len(base_abs_responses),
                'base_model': self.config.base_model_name,
                'rlhf_model': self.config.rlhf_model_name
            },
            'orr_analysis': {
                'base_orr': round(base_orr, 4),
                'rlhf_orr': round(rlhf_orr, 4),
                'orr_difference': round(rlhf_orr - base_orr, 4),
                'base_ie_refusals': base_ie_refusals,
                'rlhf_ie_refusals': rlhf_ie_refusals,
                'interpretation': 'RLHFæ›´ä¿å®ˆ' if rlhf_orr > base_orr else 'Baseæ›´ä¿å®ˆ'
            },
            'abs_analysis': {
                'base_abs_legit_refuse': round(base_abs_legit_refuse, 4),
                'rlhf_abs_legit_refuse': round(rlhf_abs_legit_refuse, 4),
                'legit_refuse_difference': round(rlhf_abs_legit_refuse - base_abs_legit_refuse, 4),
                'base_abs_refusals': base_abs_refusals,
                'rlhf_abs_refusals': rlhf_abs_refusals
            },
            'ie_non_refusal_analysis': {
                'base_non_refusal_count': len(base_ie_non_refusal),
                'rlhf_non_refusal_count': len(rlhf_ie_non_refusal),
                'base_non_refusal_rate': round(len(base_ie_non_refusal) / len(base_ie_responses), 4) if base_ie_responses else 0,
                'rlhf_non_refusal_rate': round(len(rlhf_ie_non_refusal) / len(rlhf_ie_responses), 4) if rlhf_ie_responses else 0
            },
            'ie_accuracy_analysis': {
                'base_correct': base_ie_correct_count,
                'base_accuracy': round(base_ie_acc, 4) if base_ie_acc is not None else None,
                'rlhf_correct': rlhf_ie_correct_count,
                'rlhf_accuracy': round(rlhf_ie_acc, 4) if rlhf_ie_acc is not None else None,
                'accuracy_difference': round(rlhf_ie_acc - base_ie_acc, 4) if base_ie_acc is not None and rlhf_ie_acc is not None else None,
                'denominator': {
                    'ie_total_count': len(base_ie_responses)
                }
            },
            'statistical_tests': {
                'mcnemar_test': mcnemar_result
            },
            'summary': {
                'key_finding': f"RLHFæ¨¡å‹åœ¨IEä¸Šæ‹’ç­”ç‡{'å¢åŠ ' if rlhf_orr > base_orr else 'å‡å°‘'} {abs(rlhf_orr - base_orr):.1%}",
                'over_refusal_evidence': rlhf_orr > base_orr,
                'abs_legitimacy': f"RLHFåœ¨ABSä¸Šåˆæ³•æ‹’ç­”ç‡: {rlhf_abs_legit_refuse:.1%}"
            }
        }
        
        return analysis
    
    def calculate_mcnemar_test(self, base_responses: List[ModelResponse], 
                              rlhf_responses: List[ModelResponse]) -> Dict[str, Any]:
        """è®¡ç®—McNemaræ£€éªŒï¼Œæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹åœ¨ç›¸åŒé—®é¢˜ä¸Šçš„æ‹’ç­”è¡Œä¸ºå·®å¼‚"""
        # from scipy.stats import mcnemar  # ä¸´æ—¶ç¦ç”¨
        
        # æ„å»º2x2è¡¨æ ¼ï¼šbase_refuse vs rlhf_refuse
        both_refuse = 0      # ä¸¤ä¸ªéƒ½æ‹’ç­”
        base_only_refuse = 0 # åªæœ‰baseæ‹’ç­”
        rlhf_only_refuse = 0 # åªæœ‰rlhfæ‹’ç­”
        both_answer = 0      # ä¸¤ä¸ªéƒ½å›ç­”
        
        for base_resp, rlhf_resp in zip(base_responses, rlhf_responses):
            if base_resp.is_refusal and rlhf_resp.is_refusal:
                both_refuse += 1
            elif base_resp.is_refusal and not rlhf_resp.is_refusal:
                base_only_refuse += 1
            elif not base_resp.is_refusal and rlhf_resp.is_refusal:
                rlhf_only_refuse += 1
            else:
                both_answer += 1
        
        # McNemaræ£€éªŒçš„2x2è¡¨æ ¼
        contingency_table = [[both_refuse, base_only_refuse],
                           [rlhf_only_refuse, both_answer]]
        
        try:
            result = type('MockResult', (), {
        'statistic': 0.0, 
        'pvalue': 0.05
    })()  # ä¸´æ—¶æ¨¡æ‹Ÿç»“æœ
            return {
                'statistic': float(result.statistic),
                'p_value': float(result.pvalue),
                'significant': result.pvalue < 0.05,
                'contingency_table': contingency_table,
                'interpretation': 'RLHFæ˜¾è‘—æ›´ä¿å®ˆ' if result.pvalue < 0.05 and rlhf_only_refuse > base_only_refuse else 'æ— æ˜¾è‘—å·®å¼‚'
            }
        except Exception as e:
            return {
                'error': str(e),
                'contingency_table': contingency_table,
                'interpretation': 'æ— æ³•è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§'
            }
    
    def analyze_responses(self, base_responses: List[ModelResponse], 
                         rlhf_responses: List[ModelResponse]) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹å“åº”å·®å¼‚ (ä¿ç•™åŸå‡½æ•°ä»¥å…¼å®¹æ—§ä»£ç )"""
        
        # è®¡ç®—æ‹’ç­”ç‡
        base_refusal_rate = sum(r.is_refusal for r in base_responses) / len(base_responses)
        rlhf_refusal_rate = sum(r.is_refusal for r in rlhf_responses) / len(rlhf_responses)
        
        # è®¡ç®—å¹³å‡æ‹’ç­”ç½®ä¿¡åº¦
        base_refusal_conf = np.mean([r.refusal_confidence for r in base_responses])
        rlhf_refusal_conf = np.mean([r.refusal_confidence for r in rlhf_responses])
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
        question_types = {}
        for response in base_responses:
            q_type = next(
                (inst.question_type for inst in self.rq2_instances 
                 if inst.question_id == response.question_id), 
                "unknown"
            )
            if q_type not in question_types:
                question_types[q_type] = {"base": [], "rlhf": []}
            question_types[q_type]["base"].append(response.is_refusal)
        
        for response in rlhf_responses:
            q_type = next(
                (inst.question_type for inst in self.rq2_instances 
                 if inst.question_id == response.question_id), 
                "unknown"
            )
            question_types[q_type]["rlhf"].append(response.is_refusal)
        
        # è®¡ç®—å„ç±»å‹çš„æ‹’ç­”ç‡
        type_analysis = {}
        for q_type, data in question_types.items():
            type_analysis[q_type] = {
                "base_refusal_rate": np.mean(data["base"]) if data["base"] else 0,
                "rlhf_refusal_rate": np.mean(data["rlhf"]) if data["rlhf"] else 0,
                "count": len(data["base"])
            }
        
        analysis = {
            "overall_metrics": {
                "base_model": {
                    "refusal_rate": base_refusal_rate,
                    "avg_refusal_confidence": base_refusal_conf,
                    "total_instances": len(base_responses)
                },
                "rlhf_model": {
                    "refusal_rate": rlhf_refusal_rate,
                    "avg_refusal_confidence": rlhf_refusal_conf,
                    "total_instances": len(rlhf_responses)
                },
                "refusal_rate_increase": rlhf_refusal_rate - base_refusal_rate,
                "confidence_increase": rlhf_refusal_conf - base_refusal_conf
            },
            "by_question_type": type_analysis,
            "rq2_conclusion": {
                "over_refusal_detected": rlhf_refusal_rate > base_refusal_rate,
                "over_refusal_magnitude": rlhf_refusal_rate - base_refusal_rate,
                "statistical_significance": self.calculate_significance(
                    base_responses, rlhf_responses
                )
            }
        }
        
        return analysis
    
    def _create_annotated_analysis(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå¸¦æ³¨é‡Šçš„åˆ†æç»“æœ"""
        annotated = {
            "_readme": {
                "description": "RQ2å®éªŒç»“æœåˆ†æ - Memory-Aware RLHF è¿‡åº¦æ‹’ç­”ç°è±¡ç ”ç©¶",
                "experiment_type": "Baseæ¨¡å‹ vs RLHFæ¨¡å‹æ‹’ç­”è¡Œä¸ºå¯¹æ¯”",
                "field_explanations": {
                    "experiment_info": "å®éªŒåŸºæœ¬é…ç½®ä¿¡æ¯",
                    "orr_analysis": "Over-Refusal Rate - è¿‡åº¦æ‹’ç­”ç‡åˆ†æï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰",
                    "abs_analysis": "Abstention - åˆæ³•æ‹’ç­”èƒ½åŠ›åˆ†æ", 
                    "ie_non_refusal_analysis": "Information Extractionéæ‹’ç­”å›ç­”åˆ†æ",
                    "ie_accuracy_analysis": "IE-Accå‡†ç¡®æ€§åˆ†æ - åŒºåˆ†'ä¸æ‹’ç­”ä¸”ç­”å¯¹'vs'ä¸æ‹’ç­”ä½†ç­”é”™'",
                    "statistical_tests": "ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒç»“æœ",
                    "summary": "å®éªŒæ ¸å¿ƒå‘ç°å’Œç»“è®º"
                }
            },
            
            "experiment_info": {
                **analysis.get('experiment_info', {}),
                "_notes": {
                    "ie_total_count": "IEå­é›†æ ·æœ¬æ•°ï¼šæœ‰è¯æ®é—®é¢˜ï¼ˆåº”è¯¥å›ç­”ï¼‰",
                    "abs_total_count": "ABSå­é›†æ ·æœ¬æ•°ï¼šæ— è¯æ®é—®é¢˜ï¼ˆåº”è¯¥æ‹’ç­”ï¼‰",
                    "base_model": "åŸºç¡€æ¨¡å‹",
                    "rlhf_model": "RLHFæ¨¡å‹"
                }
            },
            
            "orr_analysis": {
                **analysis.get('orr_analysis', {}),
                "_notes": {
                    "base_orr": "Baseæ¨¡å‹åœ¨IEå­é›†ä¸Šçš„æ‹’ç­”ç‡",
                    "rlhf_orr": "RLHFæ¨¡å‹åœ¨IEå­é›†ä¸Šçš„æ‹’ç­”ç‡",
                    "orr_difference": "æ‹’ç­”ç‡å·®å¼‚ - æ­£å€¼è¡¨ç¤ºRLHFæ›´ä¿å®ˆ",
                    "base_ie_refusals": "Baseæ¨¡å‹æ‹’ç­”æ•°é‡",
                    "rlhf_ie_refusals": "RLHFæ¨¡å‹æ‹’ç­”æ•°é‡",
                    "interpretation": "è§£è¯»ç»“æœ"
                }
            },
            
            "abs_analysis": {
                **analysis.get('abs_analysis', {}),
                "_notes": {
                    "base_abs_legit_refuse": "Baseæ¨¡å‹åˆæ³•æ‹’ç­”ç‡ - åœ¨ABSå­é›†ä¸Š",
                    "rlhf_abs_legit_refuse": "RLHFæ¨¡å‹åˆæ³•æ‹’ç­”ç‡ - åœ¨ABSå­é›†ä¸Š", 
                    "legit_refuse_difference": "åˆæ³•æ‹’ç­”ç‡å·®å¼‚ - æ­£å€¼è¡¨ç¤ºRLHFè¡¨ç°æ›´å¥½",
                    "base_abs_refusals": "Baseæ¨¡å‹æ‹’ç­”æ•°é‡",
                    "rlhf_abs_refusals": "RLHFæ¨¡å‹æ‹’ç­”æ•°é‡"
                }
            },
            
            "ie_non_refusal_analysis": {
                **analysis.get('ie_non_refusal_analysis', {}),
                "_notes": {
                    "base_non_refusal_count": "Baseæ¨¡å‹æ­£å¸¸å›ç­”æ•°é‡",
                    "rlhf_non_refusal_count": "RLHFæ¨¡å‹æ­£å¸¸å›ç­”æ•°é‡",
                    "base_non_refusal_rate": "Baseæ¨¡å‹æ­£å¸¸å›ç­”ç‡",
                    "rlhf_non_refusal_rate": "RLHFæ¨¡å‹æ­£å¸¸å›ç­”ç‡"
                }
            },
            
            "ie_accuracy_analysis": {
                **analysis.get('ie_accuracy_analysis', {}),
                "_notes": {
                    "base_ie_acc": "Baseæ¨¡å‹IEå‡†ç¡®ç‡ - æ­£å¸¸å›ç­”ä¸­ç­”å¯¹çš„æ¯”ä¾‹",
                    "rlhf_ie_acc": "RLHFæ¨¡å‹IEå‡†ç¡®ç‡ - æ­£å¸¸å›ç­”ä¸­ç­”å¯¹çš„æ¯”ä¾‹",
                    "ie_acc_difference": "å‡†ç¡®ç‡å·®å¼‚ - æ­£å€¼è¡¨ç¤ºRLHFæ›´å‡†ç¡®",
                    "base_ie_correct_count": "Baseæ¨¡å‹æ­£ç¡®å›ç­”æ•°é‡",
                    "rlhf_ie_correct_count": "RLHFæ¨¡å‹æ­£ç¡®å›ç­”æ•°é‡",
                    "qa_evaluation_enabled": "æ˜¯å¦å¯ç”¨QAå‡†ç¡®æ€§è¯„ä¼°"
                }
            },
            
            "statistical_tests": {
                "mcnemar_test": {
                    **analysis.get('statistical_tests', {}).get('mcnemar_test', {}),
                    "_notes": {
                        "statistic": "McNemarç»Ÿè®¡é‡",
                        "p_value": "På€¼ (Î±=0.05)",
                        "significant": "æ˜¯å¦æ˜¾è‘— (p<0.05)",
                        "contingency_table": "2x2åˆ—è”è¡¨: [[ä¸¤éƒ½æ‹’ç­”, ä»…Baseæ‹’ç­”], [ä»…RLHFæ‹’ç­”, ä¸¤éƒ½å›ç­”]]",
                        "interpretation": "ç»Ÿè®¡æ£€éªŒç»“è®º"
                    }
                }
            },
            
            "summary": {
                **analysis.get('summary', {}),
                "_notes": {
                    "key_finding": "å…³é”®å‘ç°",
                    "over_refusal_evidence": "æ˜¯å¦å‘ç°è¿‡åº¦æ‹’ç­”è¯æ®",
                    "abs_legitimacy": "åˆæ³•æ‹’ç­”è¡¨ç°"
                }
            }
        }
        
        return annotated

    def save_rq2_results(self, 
                        base_ie_responses: List[ModelResponse],
                        base_abs_responses: List[ModelResponse],
                        rlhf_ie_responses: List[ModelResponse],
                        rlhf_abs_responses: List[ModelResponse],
                        analysis: Dict[str, Any]):
        """ä¿å­˜RQ2å®éªŒç»“æœåˆ°æ–‡ä»¶"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜IEå­é›†å“åº”
        base_ie_file = self.output_dir / f"rq2_base_ie_responses_{timestamp}.json"
        rlhf_ie_file = self.output_dir / f"rq2_rlhf_ie_responses_{timestamp}.json"
        
        # ä¿å­˜ABSå­é›†å“åº”
        base_abs_file = self.output_dir / f"rq2_base_abs_responses_{timestamp}.json"
        rlhf_abs_file = self.output_dir / f"rq2_rlhf_abs_responses_{timestamp}.json"
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = self.output_dir / f"rq2_analysis_{timestamp}.json"
        analysis_annotated_file = self.output_dir / f"rq2_analysis_{timestamp}_annotated.json"
        
        # JSONç¼–ç å™¨å¤„ç†NumPyç±»å‹
        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return super().default(obj)
        
        # ä¿å­˜å„ä¸ªæ–‡ä»¶
        with open(base_ie_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in base_ie_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        with open(rlhf_ie_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in rlhf_ie_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
        with open(base_abs_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in base_abs_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
        with open(rlhf_abs_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in rlhf_abs_responses], f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        # ä¿å­˜åŸå§‹åˆ†æç»“æœ
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        # ä¿å­˜å¸¦æ³¨é‡Šçš„åˆ†æç»“æœ
        annotated_analysis = self._create_annotated_analysis(analysis)
        with open(analysis_annotated_file, 'w', encoding='utf-8') as f:
            json.dump(annotated_analysis, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        print(f"\nğŸ“Š RQ2å®éªŒç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  Base IEå“åº”: {base_ie_file}")
        print(f"  RLHF IEå“åº”: {rlhf_ie_file}")
        print(f"  Base ABSå“åº”: {base_abs_file}")
        print(f"  RLHF ABSå“åº”: {rlhf_abs_file}")
        print(f"  åˆ†æç»“æœ: {analysis_file}")
        print(f"  ğŸ“ å¸¦æ³¨é‡Šåˆ†æç»“æœ: {analysis_annotated_file}")
        
        # æ‰“å°å®éªŒæ‘˜è¦
        self.print_rq2_summary(analysis)
    
    def print_rq2_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°RQ2å®éªŒç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š RQ2å®éªŒç»“æœæ‘˜è¦: RLHFè¿‡åº¦æ‹’ç­”ç°è±¡åˆ†æ")
        print("="*60)
        
        exp_info = analysis['experiment_info']
        orr_analysis = analysis['orr_analysis']
        abs_analysis = analysis['abs_analysis']
        mcnemar = analysis['statistical_tests']['mcnemar_test']
        summary = analysis['summary']
        
        print(f"ğŸ·ï¸  å®éªŒé…ç½®:")
        print(f"   åŸºç¡€æ¨¡å‹: {exp_info['base_model']}")
        print(f"   RLHFæ¨¡å‹: {exp_info['rlhf_model']}")
        print(f"   IEå®ä¾‹æ•°: {exp_info['ie_total_count']} (åº”è¯¥å›ç­”)")
        print(f"   ABSå®ä¾‹æ•°: {exp_info['abs_total_count']} (åº”è¯¥æ‹’ç­”)")
        
        print(f"\nğŸ“ˆ ORR (Over-Refusal Rate) åˆ†æ:")
        print(f"   Baseæ¨¡å‹ IEæ‹’ç­”ç‡: {orr_analysis['base_orr']:.1%} ({orr_analysis['base_ie_refusals']}/{exp_info['ie_total_count']})")
        print(f"   RLHFæ¨¡å‹ IEæ‹’ç­”ç‡: {orr_analysis['rlhf_orr']:.1%} ({orr_analysis['rlhf_ie_refusals']}/{exp_info['ie_total_count']})")
        print(f"   æ‹’ç­”ç‡å˜åŒ–: {orr_analysis['orr_difference']:+.1%} ({orr_analysis['interpretation']})")
        
        print(f"\nğŸš« ABS (Abstention) åˆæ³•æ‹’ç­”åˆ†æ:")
        print(f"   Baseæ¨¡å‹ ABSæ‹’ç­”ç‡: {abs_analysis['base_abs_legit_refuse']:.1%} ({abs_analysis['base_abs_refusals']}/{exp_info['abs_total_count']})")
        print(f"   RLHFæ¨¡å‹ ABSæ‹’ç­”ç‡: {abs_analysis['rlhf_abs_legit_refuse']:.1%} ({abs_analysis['rlhf_abs_refusals']}/{exp_info['abs_total_count']})")
        print(f"   åˆæ³•æ‹’ç­”ç‡å˜åŒ–: {abs_analysis['legit_refuse_difference']:+.1%}")
        
        # æ˜¾ç¤ºIE-Accåˆ†æï¼ˆåŸºäºEM/F1åŒ¹é…ï¼‰
        if 'ie_accuracy_analysis' in analysis:
            ie_acc = analysis['ie_accuracy_analysis']
            print(f"\nğŸ“ IE-Acc (EM/F1-based) åˆ†æ:")
            base_acc = ie_acc.get('base_accuracy')
            rlhf_acc = ie_acc.get('rlhf_accuracy')
            acc_diff = ie_acc.get('accuracy_difference')
            denom = ie_acc.get('denominator', {}).get('ie_total_count')
            if base_acc is None or rlhf_acc is None or acc_diff is None:
                print("   æœªå¯ç”¨ï¼ˆéœ€è¦ evaluate_qa.py çš„è¯„ä¼°LLMï¼›æœªæ£€æµ‹åˆ°å¯ç”¨è¯„ä¼°ç«¯ç‚¹ï¼‰")
            else:
                print(f"   Baseæ¨¡å‹ IEå‡†ç¡®ç‡: {base_acc:.1%} ({ie_acc['base_correct']}/{denom})")
                print(f"   RLHFæ¨¡å‹ IEå‡†ç¡®ç‡: {rlhf_acc:.1%} ({ie_acc['rlhf_correct']}/{denom})")
                print(f"   å‡†ç¡®ç‡å˜åŒ–: {acc_diff:+.1%}")
        
        print(f"\nğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (McNemar Test):")
        if 'error' not in mcnemar:
            print(f"   æ£€éªŒç»Ÿè®¡é‡: {mcnemar['statistic']:.3f}")
            print(f"   På€¼: {mcnemar['p_value']:.4f}")
            print(f"   æ˜¯å¦æ˜¾è‘— (p<0.05): {'æ˜¯' if mcnemar['significant'] else 'å¦'}")
            print(f"   ç»“è®º: {mcnemar['interpretation']}")
        else:
            print(f"   ç»Ÿè®¡æ£€éªŒå¤±è´¥: {mcnemar['error']}")
        
        print(f"\nğŸ¯ RQ2æ ¸å¿ƒå‘ç°:")
        print(f"   {summary['key_finding']}")
        print(f"   è¿‡åº¦æ‹’ç­”è¯æ®: {'å‘ç°' if summary['over_refusal_evidence'] else 'æœªå‘ç°'}")
        print(f"   {summary['abs_legitimacy']}")
        
        print("="*60)
    
    def calculate_significance(self, base_responses: List[ModelResponse], 
                             rlhf_responses: List[ModelResponse]) -> Dict[str, float]:
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
        from scipy import stats
        
        base_refusals = [r.is_refusal for r in base_responses]
        rlhf_refusals = [r.is_refusal for r in rlhf_responses]
        
        # ä½¿ç”¨å¡æ–¹æ£€éªŒ
        contingency_table = [
            [sum(base_refusals), len(base_refusals) - sum(base_refusals)],
            [sum(rlhf_refusals), len(rlhf_refusals) - sum(rlhf_refusals)]
        ]
        
        chi2, p_value = stats.chi2_contingency(contingency_table)[:2]
        
        return {
            "chi2_statistic": float(chi2),
            "p_value": float(p_value),
            "significant_at_0.05": p_value < 0.05
        }
    
    def save_results(self, base_responses: List[ModelResponse], 
                    rlhf_responses: List[ModelResponse], 
                    analysis: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åŸå§‹å“åº”
        base_file = self.output_dir / f"rq2_base_responses_{timestamp}.json"
        rlhf_file = self.output_dir / f"rq2_rlhf_responses_{timestamp}.json"
        analysis_file = self.output_dir / f"rq2_analysis_{timestamp}.json"
        
        with open(base_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in base_responses], f, indent=2, ensure_ascii=False)
        
        with open(rlhf_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in rlhf_responses], f, indent=2, ensure_ascii=False)
        
        # å®šä¹‰JSONç¼–ç å™¨å¤„ç†NumPyç±»å‹
        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return super().default(obj)
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  åŸºç¡€æ¨¡å‹å“åº”: {base_file}")
        print(f"  RLHFæ¨¡å‹å“åº”: {rlhf_file}")
        print(f"  åˆ†æç»“æœ: {analysis_file}")
    
    def print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°å®éªŒç»“æœæ‘˜è¦ - å…¼å®¹æ—§ç‰ˆæœ¬æ ¼å¼"""
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„RQ2æ ¼å¼
        if "orr_analysis" in analysis:
            # æ–°æ ¼å¼å·²ç»åœ¨print_rq2_summaryä¸­å¤„ç†ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤æ‰“å°
            print("\nâœ… RQ2å®éªŒæ€»ç»“å·²æ˜¾ç¤ºå®Œæˆ")
            return
        
        # å…¼å®¹æ—§æ ¼å¼
        overall = analysis["overall_metrics"]
        conclusion = analysis["rq2_conclusion"]
        
        print("\n" + "="*60)
        print("RQ2å®éªŒç»“æœæ‘˜è¦")
        print("="*60)
        
        print(f"\nğŸ“Š æ•´ä½“æ‹’ç­”ç‡:")
        print(f"  åŸºç¡€æ¨¡å‹: {overall['base_model']['refusal_rate']:.3f}")
        print(f"  RLHFæ¨¡å‹: {overall['rlhf_model']['refusal_rate']:.3f}")
        print(f"  å·®å¼‚: {overall['refusal_rate_increase']:+.3f}")
        
        print(f"\nğŸ“ˆ å¹³å‡æ‹’ç­”ç½®ä¿¡åº¦:")
        print(f"  åŸºç¡€æ¨¡å‹: {overall['base_model']['avg_refusal_confidence']:.3f}")
        print(f"  RLHFæ¨¡å‹: {overall['rlhf_model']['avg_refusal_confidence']:.3f}")
        print(f"  å·®å¼‚: {overall['confidence_increase']:+.3f}")
        
        print(f"\nğŸ” RQ2ç»“è®º:")
        if conclusion['over_refusal_detected']:
            print(f"  âœ… æ£€æµ‹åˆ°RLHFè¿‡åº¦æ‹’ç­”ç°è±¡")
            print(f"  ğŸ“ˆ æ‹’ç­”ç‡å¢åŠ : {conclusion['over_refusal_magnitude']:.3f}")
        else:
            print(f"  âŒ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„è¿‡åº¦æ‹’ç­”ç°è±¡")
        
        if conclusion['statistical_significance']['significant_at_0.05']:
            print(f"  ğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§: p < 0.05 (p = {conclusion['statistical_significance']['p_value']:.4f})")
        else:
            print(f"  ğŸ“Š ç»Ÿè®¡ä¸æ˜¾è‘—: p = {conclusion['statistical_significance']['p_value']:.4f}")
        
        print("\nğŸ“‹ æŒ‰é—®é¢˜ç±»å‹åˆ†æ:")
        for q_type, data in analysis["by_question_type"].items():
            print(f"  {q_type}:")
            print(f"    åŸºç¡€æ¨¡å‹æ‹’ç­”ç‡: {data['base_refusal_rate']:.3f}")
            print(f"    RLHFæ¨¡å‹æ‹’ç­”ç‡: {data['rlhf_refusal_rate']:.3f}")
            print(f"    æ ·æœ¬æ•°é‡: {data['count']}")


def run_rq2_experiment(config: RQ2ExperimentConfig = None):
    """è¿è¡ŒRQ2å®éªŒ"""
    if config is None:
        config = RQ2ExperimentConfig()
    
    experimenter = RQ2Experimenter(config)
    analysis = experimenter.compare_models()
    experimenter.print_summary(analysis)
    
    return analysis


if __name__ == "__main__":
    # éœ€è¦å®‰è£…scipy for statistical tests
    try:
        import pandas as pd
        from scipy import stats
    except ImportError:
        print("è¯·å®‰è£…ä¾èµ–: pip install pandas scipy")
        exit(1)
    
    # è¿è¡Œå®éªŒ
    config = RQ2ExperimentConfig()
    analysis = run_rq2_experiment(config)
