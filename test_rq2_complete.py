#!/usr/bin/env python3
"""
RQ2å®éªŒå®Œæ•´æµ‹è¯•å¥—ä»¶
====================================

åŠŸèƒ½æ¦‚è¿°:
    é›†æˆæ‰€æœ‰RQ2å®éªŒç›¸å…³çš„æµ‹è¯•åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ¡†æ¶æµ‹è¯•ã€é…ç½®æµ‹è¯•ã€æ•°æ®éªŒè¯ç­‰ã€‚
    æä¾›ä¸€ç«™å¼çš„å®éªŒå‰æ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£å¸¸å·¥ä½œã€‚

ä¸»è¦æµ‹è¯•æ¨¡å—:
1. **æ¡†æ¶åŸºç¡€æµ‹è¯•**
   - æ•°æ®åŠ è½½å’Œå¤„ç†
   - æ‹’ç­”æ£€æµ‹ç®—æ³•
   - æç¤ºç”Ÿæˆé€»è¾‘

2. **é…ç½®å’Œæ¨¡å‹æµ‹è¯•**
   - å®éªŒé…ç½®åŠ è½½
   - æ¨¡å‹é…ç½®éªŒè¯
   - æ•°æ®é›†å¯ç”¨æ€§æ£€æŸ¥

3. **å®éªŒå°±ç»ªæ€§æ£€æŸ¥**
   - ç¯å¢ƒä¾èµ–éªŒè¯
   - GPUå¯ç”¨æ€§æ£€æµ‹
   - å‘½ä»¤è¡Œä½¿ç”¨æŒ‡å—

ä½¿ç”¨æ–¹æ³•:
    ```bash
    # å®Œæ•´æµ‹è¯•
    python test_rq2_complete.py
    
    # åªæµ‹è¯•ç‰¹å®šæ¨¡å—
    python test_rq2_complete.py --framework-only
    python test_rq2_complete.py --config-only
    ```

æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æµ‹è¯•ï¼Œä¸ä¼šåŠ è½½å¤§æ¨¡å‹ï¼Œé€‚åˆå¿«é€ŸéªŒè¯ã€‚
"""

import sys
import os
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ä¾èµ–æ£€æŸ¥
DEPS_AVAILABLE = True
try:
    import torch
    import numpy as np
    from transformers import AutoTokenizer
    from tqdm import tqdm
except ImportError as e:
    print(f"âš ï¸ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    DEPS_AVAILABLE = False


def test_environment():
    """æµ‹è¯•ç¯å¢ƒé…ç½®"""
    print("ğŸŒ ç¯å¢ƒé…ç½®æµ‹è¯•...")
    
    # Pythonç‰ˆæœ¬
    print(f"  Pythonç‰ˆæœ¬: {sys.version}")
    
    # ä¾èµ–åŒ…æ£€æŸ¥
    if DEPS_AVAILABLE:
        print("  âœ… æ ¸å¿ƒä¾èµ–åŒ…å¯ç”¨")
        
        # GPUæ£€æŸ¥
        if torch.cuda.is_available():
            print(f"  âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"  ğŸ“Š æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("  âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿è¡Œ")
    else:
        print("  âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åŒ…")
        return False
    
    return True


def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nğŸ“ æ•°æ®åŠ è½½æµ‹è¯•...")
    
    try:
        from data.longmemeval_loader import LongMemEvalLoader
        
        # æµ‹è¯•æ•°æ®è·¯å¾„
        data_path = "/mnt/d/datasets/longmemeval_data/longmemeval_oracle.json"
        
        if not os.path.exists(data_path):
            print(f"  âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return False
        
        # åŠ è½½æ•°æ®
        loader = LongMemEvalLoader(data_path)
        all_instances = loader.load_data()
        rq2_instances = loader.get_rq2_instances()
        abs_instances = loader.get_abstention_instances()
        
        print(f"  âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"    æ€»å®ä¾‹æ•°: {len(all_instances)}")
        print(f"    RQ2å®ä¾‹æ•°: {len(rq2_instances)}")
        print(f"    æ‹’ç­”å®ä¾‹æ•°: {len(abs_instances)}")
        
        # é—®é¢˜ç±»å‹ç»Ÿè®¡
        question_types = {}
        for instance in all_instances:
            q_type = instance.question_type
            question_types[q_type] = question_types.get(q_type, 0) + 1
        
        print(f"  ğŸ“Š é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        for q_type, count in sorted(question_types.items()):
            print(f"    {q_type}: {count}")
        
        # æµ‹è¯•æ ¼å¼åŒ–åŠŸèƒ½
        if rq2_instances:
            sample = rq2_instances[0]
            formatted = loader.format_conversation_history(sample, max_sessions=2)
            print(f"  ğŸ“ æ ·ä¾‹é—®é¢˜: {sample.question[:50]}...")
            print(f"    æ ¼å¼åŒ–å†å²é•¿åº¦: {len(formatted)} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False


def test_refusal_detection():
    """æµ‹è¯•æ‹’ç­”æ£€æµ‹"""
    print("\nğŸ” æ‹’ç­”æ£€æµ‹æµ‹è¯•...")
    
    if not DEPS_AVAILABLE:
        print("  âš ï¸ ä¾èµ–ä¸å®Œæ•´ï¼Œè·³è¿‡æ‹’ç­”æ£€æµ‹æµ‹è¯•")
        return False
    
    try:
        from utils.refusal_detector import RefusalDetector
        
        detector = RefusalDetector()
        
        # æµ‹è¯•æ¡ˆä¾‹
        test_cases = [
            ("I don't know the answer to that question.", True),    # æ˜ç¡®æ‹’ç­”
            ("I'm not sure about this information.", True),         # ä¸ç¡®å®š
            ("The weather is sunny today.", False),                 # æ­£å¸¸å›ç­”
            ("Based on the data, the result is 42.", False),       # æ­£å¸¸å›ç­”
            ("I cannot find this information.", True),              # æ‹’ç­”
            ("There is no mention of this in the context.", True), # æ‹’ç­”
        ]
        
        correct = 0
        total = len(test_cases)
        
        print(f"  ğŸ“‹ æµ‹è¯• {total} ä¸ªæ¡ˆä¾‹:")
        for i, (text, expected) in enumerate(test_cases, 1):
            is_refusal, confidence = detector.detect_refusal(text)
            is_correct = (is_refusal == expected)
            correct += is_correct
            
            status = "âœ…" if is_correct else "âŒ"
            print(f"    {i}. {status} æ£€æµ‹: {is_refusal}, ç½®ä¿¡åº¦: {confidence:.3f}")
            print(f"       æ–‡æœ¬: {text[:40]}...")
        
        accuracy = correct / total
        print(f"  ğŸ¯ å‡†ç¡®ç‡: {accuracy:.1%} ({correct}/{total})")
        
        return accuracy > 0.7
        
    except Exception as e:
        print(f"  âŒ æ‹’ç­”æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\nâš™ï¸ é…ç½®åŠ è½½æµ‹è¯•...")
    
    try:
        from experiments.config import get_rq2_config, list_available_configs, MODELS
        
        # æµ‹è¯•æ¨¡å‹é…ç½®
        print(f"  ğŸ“Š å¯ç”¨æ¨¡å‹æ•°é‡: {len(MODELS)}")
        for model_name, model_config in MODELS.items():
            print(f"    {model_name}: {model_config.path}")
        
        # æµ‹è¯•RQ2é…ç½®
        configs_to_test = ["qwen2.5-3b", "llama3.2-3b", "mistral-7b", "long_context"]
        success_count = 0
        
        for config_name in configs_to_test:
            try:
                config = get_rq2_config(config_name)
                print(f"  âœ… {config_name}: {config['description']}")
                print(f"    æ¨¡å‹å¯¹æ•°: {len(config['model_pairs'])}")
                success_count += 1
            except Exception as e:
                print(f"  âŒ {config_name}: {e}")
        
        print(f"  ğŸ¯ é…ç½®æˆåŠŸç‡: {success_count}/{len(configs_to_test)}")
        return success_count == len(configs_to_test)
        
    except Exception as e:
        print(f"  âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False


def test_prompt_generation():
    """æµ‹è¯•æç¤ºç”Ÿæˆ"""
    print("\nğŸ“ æç¤ºç”Ÿæˆæµ‹è¯•...")
    
    try:
        from data.longmemeval_loader import LongMemEvalLoader
        
        data_path = "/mnt/d/datasets/longmemeval_data/longmemeval_oracle.json"
        if not os.path.exists(data_path):
            print("  âš ï¸ è·³è¿‡ï¼šæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return True
        
        loader = LongMemEvalLoader(data_path)
        rq2_instances = loader.get_rq2_instances()
        
        if not rq2_instances:
            print("  âš ï¸ è·³è¿‡ï¼šæ²¡æœ‰RQ2å®ä¾‹")
            return True
        
        # æµ‹è¯•æç¤ºç”Ÿæˆ
        sample = rq2_instances[0]
        
        # æµ‹è¯•ä¸åŒçš„ä¼šè¯æ•°é™åˆ¶
        for max_sessions in [2, 5, None]:
            history = loader.format_conversation_history(sample, max_sessions)
            
            # ç®€å•çš„æç¤ºæ¨¡æ¿
            prompt = f"""Based on the conversation history, please answer: {sample.question}

Conversation History:
{history}

Answer:"""
            
            prompt_len = len(prompt)
            print(f"  ğŸ“ max_sessions={max_sessions}: {prompt_len} å­—ç¬¦")
            
            if max_sessions == 2:
                print(f"    æ ·ä¾‹ç‰‡æ®µ: {prompt[:100]}...")
        
        print("  âœ… æç¤ºç”Ÿæˆæ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"  âŒ æç¤ºç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def show_usage_guide():
    """æ˜¾ç¤ºä½¿ç”¨æŒ‡å—"""
    print("\n" + "="*60)
    print("ğŸ“– RQ2å®éªŒä½¿ç”¨æŒ‡å—")
    print("="*60)
    
    print("""
ğŸš€ æ¨èçš„å®éªŒæµç¨‹:

1. å¿«é€Ÿæµ‹è¯• Qwen2.5-3B:
   python run_rq2_experiment.py --config qwen2.5-3b --quick-test

2. å®Œæ•´ Qwen2.5-3B å®éªŒ:
   python run_rq2_experiment.py --config qwen2.5-3b

3. æµ‹è¯•å…¶ä»–æ¨¡å‹:
   python run_rq2_experiment.py --config llama3.2-3b
   python run_rq2_experiment.py --config mistral-7b  # äº‘ä¸Šè¿è¡Œ

4. ç»¼åˆå®éªŒ (æ‰€æœ‰æ¨¡å‹):
   python run_rq2_experiment.py --comprehensive

5. è‡ªå®šä¹‰æ¨¡å‹å¯¹:
   python run_rq2_experiment.py --model-pair "Qwen/Qwen2.5-3B,Qwen/Qwen2.5-3B-Instruct"

ğŸ“‹ å…¶ä»–æœ‰ç”¨å‘½ä»¤:

- æŸ¥çœ‹é…ç½®: python run_rq2_experiment.py --list-configs
- æ¡†æ¶æµ‹è¯•: python run_rq2_experiment.py --test-only
- æŸ¥çœ‹å¸®åŠ©: python run_rq2_experiment.py --help
""")


def run_complete_test():
    """è¿è¡Œå®Œæ•´æµ‹è¯•"""
    print("ğŸ§ª RQ2å®éªŒå®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("="*50)
    
    test_results = []
    
    # 1. ç¯å¢ƒæµ‹è¯•
    test_results.append(("ç¯å¢ƒé…ç½®", test_environment()))
    
    # 2. æ•°æ®åŠ è½½æµ‹è¯•
    test_results.append(("æ•°æ®åŠ è½½", test_data_loading()))
    
    # 3. æ‹’ç­”æ£€æµ‹æµ‹è¯•
    test_results.append(("æ‹’ç­”æ£€æµ‹", test_refusal_detection()))
    
    # 4. é…ç½®åŠ è½½æµ‹è¯•
    test_results.append(("é…ç½®åŠ è½½", test_config_loading()))
    
    # 5. æç¤ºç”Ÿæˆæµ‹è¯•
    test_results.append(("æç¤ºç”Ÿæˆ", test_prompt_generation()))
    
    # æ€»ç»“ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*50)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ æ€»ä½“é€šè¿‡ç‡: {passed}/{total} ({passed/total:.1%})")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RQ2å®éªŒå·²å‡†å¤‡å°±ç»ªã€‚")
        show_usage_guide()
        return True
    else:
        print(f"\nâš ï¸ {total-passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®åé‡è¯•ã€‚")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RQ2å®éªŒå®Œæ•´æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--framework-only", action="store_true",
                       help="åªè¿è¡Œæ¡†æ¶æµ‹è¯•ï¼ˆæ•°æ®ã€æ‹’ç­”æ£€æµ‹ã€æç¤ºç”Ÿæˆï¼‰")
    parser.add_argument("--config-only", action="store_true",
                       help="åªè¿è¡Œé…ç½®æµ‹è¯•ï¼ˆç¯å¢ƒã€é…ç½®åŠ è½½ï¼‰")
    parser.add_argument("--usage", action="store_true",
                       help="åªæ˜¾ç¤ºä½¿ç”¨æŒ‡å—")
    
    args = parser.parse_args()
    
    if args.usage:
        show_usage_guide()
        return
    
    if args.framework_only:
        print("ğŸ”§ è¿è¡Œæ¡†æ¶æµ‹è¯•...")
        results = [
            test_data_loading(),
            test_refusal_detection(),
            test_prompt_generation()
        ]
        success = all(results)
    elif args.config_only:
        print("âš™ï¸ è¿è¡Œé…ç½®æµ‹è¯•...")
        results = [
            test_environment(),
            test_config_loading()
        ]
        success = all(results)
    else:
        success = run_complete_test()
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
