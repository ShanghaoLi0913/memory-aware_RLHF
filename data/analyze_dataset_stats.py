#!/usr/bin/env python3
"""
LongMemEvalæ•°æ®é›†ç»Ÿè®¡åˆ†æå·¥å…·
=====================================

åŠŸèƒ½æ¦‚è¿°:
    æœ¬è„šæœ¬ç”¨äºåˆ†æLongMemEvalæ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬é—®é¢˜ç±»å‹åˆ†å¸ƒã€
    æ•°æ®è§„æ¨¡ã€å¯¹è¯å†å²é•¿åº¦ç­‰å…³é”®æŒ‡æ ‡ã€‚

ä¸»è¦ç»Ÿè®¡åŠŸèƒ½:
1. **é—®é¢˜ç±»å‹åˆ†å¸ƒ** (Question Type Distribution)
   - ç»Ÿè®¡æ¯ç§question_typeçš„æ•°é‡å’Œå æ¯”
   - åŒºåˆ†æ™®é€šé—®é¢˜å’ŒAbstentioné—®é¢˜(_absç»“å°¾)

2. **æ•°æ®è§„æ¨¡ç»Ÿè®¡** (Data Scale Statistics)
   - æ€»é—®é¢˜æ•°é‡
   - å¹³å‡å¯¹è¯å†å²é•¿åº¦
   - æœ€é•¿/æœ€çŸ­å¯¹è¯å†å²
   - Tokenæ•°é‡ä¼°ç®—

3. **æ—¶é—´åˆ†å¸ƒåˆ†æ** (Temporal Distribution)
   - é—®é¢˜æ—¥æœŸåˆ†å¸ƒ
   - å†å²ä¼šè¯æ—¶é—´è·¨åº¦

4. **ä¼šè¯ç»Ÿè®¡** (Session Statistics)
   - å¹³å‡ä¼šè¯æ•°é‡
   - è¯æ®ä¼šè¯æ¯”ä¾‹
   - ä¼šè¯é•¿åº¦åˆ†å¸ƒ

ä½¿ç”¨æ–¹æ³•:
    ```bash
    # åˆ†æé»˜è®¤æ•°æ®é›†
    python analyze_dataset_stats.py

    # åˆ†ææŒ‡å®šæ•°æ®é›†
    python analyze_dataset_stats.py --data-file data/longmemeval_data/longmemeval_s.json

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    python analyze_dataset_stats.py --save-report
    ```

è¾“å‡ºç»“æœ:
- æ§åˆ¶å°æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
- å¯é€‰ä¿å­˜è¯¦ç»†JSONæŠ¥å‘Šåˆ°results/ç›®å½•
"""

import json
import argparse
import os
from collections import Counter, defaultdict
from datetime import datetime
from typing import Dict, List, Any, Tuple
from pathlib import Path
import sys

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from data.longmemeval_loader import LongMemEvalLoader

class DatasetAnalyzer:
    """LongMemEvalæ•°æ®é›†ç»Ÿè®¡åˆ†æå™¨"""
    
    def __init__(self, data_file: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        """
        self.data_file = data_file
        self.loader = LongMemEvalLoader(data_file)
        self.instances = self.loader.load_data()
        
    def analyze_question_types(self) -> Dict[str, Any]:
        """åˆ†æé—®é¢˜ç±»å‹åˆ†å¸ƒ"""
        print("ğŸ“Š åˆ†æé—®é¢˜ç±»å‹åˆ†å¸ƒ...")
        
        # ç»Ÿè®¡é—®é¢˜ç±»å‹
        question_types = Counter()
        abstention_types = Counter()
        
        for instance in self.instances:
            q_type = instance.question_type
            question_types[q_type] += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºAbstentioné—®é¢˜
            if instance.question_id.endswith('_abs'):
                abstention_types[q_type] += 1
        
        total_questions = len(self.instances)
        total_abstention = sum(abstention_types.values())
        
        # è®¡ç®—æ¯”ä¾‹
        type_stats = {}
        for q_type, count in question_types.items():
            abs_count = abstention_types.get(q_type, 0)
            regular_count = count - abs_count
            
            type_stats[q_type] = {
                'total': count,
                'regular': regular_count,
                'abstention': abs_count,
                'percentage': round(count / total_questions * 100, 2)
            }
        
        return {
            'total_questions': total_questions,
            'total_abstention': total_abstention,
            'regular_questions': total_questions - total_abstention,
            'question_types': dict(question_types),
            'abstention_types': dict(abstention_types),
            'detailed_stats': type_stats
        }
    
    def analyze_session_statistics(self) -> Dict[str, Any]:
        """åˆ†æä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š åˆ†æä¼šè¯ç»Ÿè®¡ä¿¡æ¯...")
        
        session_counts = []
        evidence_session_counts = []
        session_lengths = []  # æ¯ä¸ªä¼šè¯çš„å¯¹è¯è½®æ•°
        haystack_session_counts = []  # æ¯ä¸ªé—®é¢˜çš„haystack_sessionsæ•°é‡
        haystack_token_lengths = []  # æ¯ä¸ªé—®é¢˜çš„haystack_sessionsæ€»tokené•¿åº¦
        
        for instance in self.instances:
            # ä¼šè¯æ•°é‡
            num_sessions = len(instance.haystack_sessions)
            session_counts.append(num_sessions)
            
            # haystack_sessionsæ•°é‡ï¼ˆæ¯ä¸ªé—®é¢˜çš„ä¼šè¯æ•°é‡ï¼‰
            haystack_session_counts.append(num_sessions)
            
            # è®¡ç®—haystack_sessionsçš„æ€»tokené•¿åº¦
            total_haystack_chars = 0
            for session in instance.haystack_sessions:
                for turn in session:
                    content = str(turn.get('content', ''))
                    total_haystack_chars += len(content)
            
            # ä¼°ç®—tokenæ•°é‡ (1 token â‰ˆ 4 characters)
            estimated_tokens = total_haystack_chars // 4
            haystack_token_lengths.append(estimated_tokens)
            
            # è¯æ®ä¼šè¯æ•°é‡
            num_evidence = len(instance.answer_session_ids)
            evidence_session_counts.append(num_evidence)
            
            # ä¼šè¯é•¿åº¦ï¼ˆå¯¹è¯è½®æ•°ï¼‰
            for session in instance.haystack_sessions:
                session_lengths.append(len(session))
        
        return {
            'sessions_per_question': {
                'min': min(session_counts),
                'max': max(session_counts),
                'avg': round(sum(session_counts) / len(session_counts), 2),
                'total_sessions': sum(session_counts)
            },
            'haystack_sessions_count': {
                'min': min(haystack_session_counts),
                'max': max(haystack_session_counts),
                'avg': round(sum(haystack_session_counts) / len(haystack_session_counts), 2),
                'median': sorted(haystack_session_counts)[len(haystack_session_counts)//2],
                'total_questions': len(haystack_session_counts)
            },
            'haystack_sessions_token_length': {
                'min': min(haystack_token_lengths),
                'max': max(haystack_token_lengths),
                'avg': round(sum(haystack_token_lengths) / len(haystack_token_lengths), 2),
                'median': sorted(haystack_token_lengths)[len(haystack_token_lengths)//2],
                'percentile_75': sorted(haystack_token_lengths)[int(len(haystack_token_lengths) * 0.75)],
                'percentile_90': sorted(haystack_token_lengths)[int(len(haystack_token_lengths) * 0.9)],
                'total_tokens': sum(haystack_token_lengths)
            },
            'evidence_sessions_per_question': {
                'min': min(evidence_session_counts),
                'max': max(evidence_session_counts),
                'avg': round(sum(evidence_session_counts) / len(evidence_session_counts), 2),
                'total_evidence_sessions': sum(evidence_session_counts)
            },
            'turns_per_session': {
                'min': min(session_lengths),
                'max': max(session_lengths),
                'avg': round(sum(session_lengths) / len(session_lengths), 2),
                'total_turns': sum(session_lengths)
            }
        }
    
    def analyze_text_statistics(self) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆé•¿åº¦ã€Tokenä¼°ç®—ç­‰ï¼‰"""
        print("ğŸ“Š åˆ†ææ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯...")
        
        question_lengths = []
        answer_lengths = []
        context_lengths = []  # æ•´ä¸ªå¯¹è¯å†å²çš„é•¿åº¦
        
        for instance in self.instances:
            # é—®é¢˜é•¿åº¦
            question_lengths.append(len(str(instance.question)))
            
            # ç­”æ¡ˆé•¿åº¦ - æ·»åŠ ç±»å‹æ£€æŸ¥
            answer_text = str(instance.answer) if instance.answer is not None else ""
            answer_lengths.append(len(answer_text))
            
            # å¯¹è¯å†å²é•¿åº¦ï¼ˆæ‰€æœ‰ä¼šè¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼‰
            total_context_length = 0
            for session in instance.haystack_sessions:
                for turn in session:
                    content = str(turn.get('content', ''))
                    total_context_length += len(content)
            context_lengths.append(total_context_length)
        
        # ç®€å•çš„Tokenä¼°ç®—ï¼ˆ1 token â‰ˆ 4 characters for Chinese/English mixï¼‰
        def estimate_tokens(char_length):
            return char_length // 4
        
        return {
            'question_lengths': {
                'min_chars': min(question_lengths),
                'max_chars': max(question_lengths),
                'avg_chars': round(sum(question_lengths) / len(question_lengths), 2),
                'avg_tokens_est': round(sum(question_lengths) / len(question_lengths) / 4, 2)
            },
            'answer_lengths': {
                'min_chars': min(answer_lengths),
                'max_chars': max(answer_lengths),
                'avg_chars': round(sum(answer_lengths) / len(answer_lengths), 2),
                'avg_tokens_est': round(sum(answer_lengths) / len(answer_lengths) / 4, 2)
            },
            'context_lengths': {
                'min_chars': min(context_lengths),
                'max_chars': max(context_lengths),
                'avg_chars': round(sum(context_lengths) / len(context_lengths), 2),
                'avg_tokens_est': round(sum(context_lengths) / len(context_lengths) / 4, 2)
            }
        }
    
    def analyze_temporal_distribution(self) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´åˆ†å¸ƒ"""
        print("ğŸ“Š åˆ†ææ—¶é—´åˆ†å¸ƒ...")
        
        from dateutil import parser
        
        question_dates = []
        session_date_ranges = []
        
        for instance in self.instances:
            try:
                # é—®é¢˜æ—¥æœŸ
                if instance.question_date:
                    try:
                        parsed_date = parser.parse(instance.question_date)
                        question_dates.append(parsed_date)
                    except:
                        pass  # è·³è¿‡æ— æ³•è§£æçš„æ—¥æœŸ
                
                # ä¼šè¯æ—¶é—´è·¨åº¦
                if instance.haystack_dates and len(instance.haystack_dates) > 1:
                    try:
                        parsed_dates = []
                        for date_str in instance.haystack_dates:
                            if date_str:
                                parsed_dates.append(parser.parse(date_str))
                        
                        if len(parsed_dates) > 1:
                            parsed_dates.sort()
                            time_span = (parsed_dates[-1] - parsed_dates[0]).days
                            session_date_ranges.append(time_span)
                    except:
                        pass  # è·³è¿‡æ— æ³•è§£æçš„æ—¥æœŸ
            except:
                continue  # è·³è¿‡æœ‰é—®é¢˜çš„å®ä¾‹
        
        return {
            'question_date_range': {
                'earliest': min(question_dates).isoformat() if question_dates else None,
                'latest': max(question_dates).isoformat() if question_dates else None,
                'total_days': (max(question_dates) - min(question_dates)).days if len(question_dates) > 1 else 0
            },
            'session_time_spans': {
                'min_days': min(session_date_ranges) if session_date_ranges else 0,
                'max_days': max(session_date_ranges) if session_date_ranges else 0,
                'avg_days': round(sum(session_date_ranges) / len(session_date_ranges), 2) if session_date_ranges else 0,
                'total_valid_spans': len(session_date_ranges)
            }
        }
    
    def generate_full_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„ç»Ÿè®¡æŠ¥å‘Š"""
        print(f"\nğŸ” å¼€å§‹åˆ†ææ•°æ®é›†: {self.data_file}")
        print(f"ğŸ“ åŠ è½½äº† {len(self.instances)} ä¸ªå®ä¾‹")
        
        report = {
            'dataset_info': {
                'file_path': self.data_file,
                'total_instances': len(self.instances),
                'analysis_time': datetime.now().isoformat()
            },
            'question_type_analysis': self.analyze_question_types(),
            'session_statistics': self.analyze_session_statistics(),
            'text_statistics': self.analyze_text_statistics(),
            'temporal_distribution': self.analyze_temporal_distribution()
        }
        
        return report
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š LongMemEvalæ•°æ®é›†ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        dataset_info = report['dataset_info']
        print(f"ğŸ“ æ•°æ®é›†æ–‡ä»¶: {dataset_info['file_path']}")
        print(f"ğŸ“Š æ€»å®ä¾‹æ•°: {dataset_info['total_instances']}")
        
        # é—®é¢˜ç±»å‹åˆ†å¸ƒ
        qt_analysis = report['question_type_analysis']
        print(f"\nğŸ·ï¸  é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        print(f"   æ€»é—®é¢˜æ•°: {qt_analysis['total_questions']}")
        print(f"   æ™®é€šé—®é¢˜: {qt_analysis['regular_questions']}")
        print(f"   Abstentioné—®é¢˜: {qt_analysis['total_abstention']}")
        
        print(f"\n   å„ç±»å‹è¯¦ç»†åˆ†å¸ƒ:")
        for q_type, stats in qt_analysis['detailed_stats'].items():
            print(f"   â€¢ {q_type}: {stats['total']} ({stats['percentage']}%)")
            print(f"     - æ™®é€š: {stats['regular']}, Abstention: {stats['abstention']}")
        
        # ä¼šè¯ç»Ÿè®¡
        session_stats = report['session_statistics']
        print(f"\nğŸ’¬ ä¼šè¯ç»Ÿè®¡:")
        print(f"   å¹³å‡ä¼šè¯æ•°/é—®é¢˜: {session_stats['sessions_per_question']['avg']}")
        print(f"   å¹³å‡è¯æ®ä¼šè¯æ•°/é—®é¢˜: {session_stats['evidence_sessions_per_question']['avg']}")
        print(f"   å¹³å‡å¯¹è¯è½®æ•°/ä¼šè¯: {session_stats['turns_per_session']['avg']}")
        
        # haystack_sessionsæ•°é‡åˆ†å¸ƒ
        haystack_count_stats = session_stats['haystack_sessions_count']
        print(f"\nğŸ“š Haystack Sessionsæ•°é‡åˆ†å¸ƒ:")
        print(f"   æœ€å°‘ä¼šè¯æ•°: {haystack_count_stats['min']}")
        print(f"   æœ€å¤šä¼šè¯æ•°: {haystack_count_stats['max']}")
        print(f"   å¹³å‡ä¼šè¯æ•°: {haystack_count_stats['avg']}")
        print(f"   ä¸­ä½æ•°ä¼šè¯æ•°: {haystack_count_stats['median']}")
        
        # haystack_sessions tokené•¿åº¦åˆ†å¸ƒ
        haystack_token_stats = session_stats['haystack_sessions_token_length']
        print(f"\nğŸ”¢ Haystack Sessions Tokené•¿åº¦åˆ†å¸ƒ:")
        print(f"   æœ€çŸ­tokené•¿åº¦: {haystack_token_stats['min']:,}")
        print(f"   æœ€é•¿tokené•¿åº¦: {haystack_token_stats['max']:,}")
        print(f"   å¹³å‡tokené•¿åº¦: {haystack_token_stats['avg']:,.2f}")
        print(f"   ä¸­ä½æ•°tokené•¿åº¦: {haystack_token_stats['median']:,}")
        print(f"   75%åˆ†ä½æ•°: {haystack_token_stats['percentile_75']:,}")
        print(f"   90%åˆ†ä½æ•°: {haystack_token_stats['percentile_90']:,}")
        
        # æ–‡æœ¬ç»Ÿè®¡
        text_stats = report['text_statistics']
        print(f"\nğŸ“ æ–‡æœ¬ç»Ÿè®¡:")
        print(f"   å¹³å‡é—®é¢˜é•¿åº¦: {text_stats['question_lengths']['avg_chars']} å­—ç¬¦ (~{text_stats['question_lengths']['avg_tokens_est']} tokens)")
        print(f"   å¹³å‡ç­”æ¡ˆé•¿åº¦: {text_stats['answer_lengths']['avg_chars']} å­—ç¬¦ (~{text_stats['answer_lengths']['avg_tokens_est']} tokens)")
        print(f"   å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦: {text_stats['context_lengths']['avg_chars']} å­—ç¬¦ (~{text_stats['context_lengths']['avg_tokens_est']} tokens)")
        
        print("\n" + "="*60)

def main():
    parser = argparse.ArgumentParser(description="LongMemEvalæ•°æ®é›†ç»Ÿè®¡åˆ†æ")
    parser.add_argument(
        "--data-file", 
        default="data/longmemeval_data/longmemeval_oracle.json",
        help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--save-report", 
        action="store_true",
        help="ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°JSONæ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
        sys.exit(1)
    
    try:
        # åˆ›å»ºåˆ†æå™¨å¹¶ç”ŸæˆæŠ¥å‘Š
        analyzer = DatasetAnalyzer(args.data_file)
        report = analyzer.generate_full_report()
        
        # æ‰“å°æ‘˜è¦
        analyzer.print_summary(report)
        
        # å¯é€‰ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        if args.save_report:
            # åˆ›å»ºresultsç›®å½•
            results_dir = Path("results")
            results_dir.mkdir(exist_ok=True)
            
            # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            dataset_name = Path(args.data_file).stem
            report_file = results_dir / f"dataset_analysis_{dataset_name}_{timestamp}.json"
            
            # ä¿å­˜æŠ¥å‘Š
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        print(f"\nâœ… æ•°æ®é›†åˆ†æå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
