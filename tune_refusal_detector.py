"""
æ‹’ç­”æ£€æµ‹å™¨è°ƒä¼˜è„šæœ¬ - Abstention Setä¸“ç”¨æµ‹è¯•
==========================================

## æ ¸å¿ƒæ¦‚å¿µ & èƒŒæ™¯
æœ¬è„šæœ¬æ˜¯Memory-aware RLHFç ”ç©¶é¡¹ç›®çš„å…³é”®ç»„ä»¶ï¼Œä¸“é—¨ç”¨äºè°ƒä¼˜å’ŒéªŒè¯æ‹’ç­”æ£€æµ‹ç®—æ³•ã€‚

### ä»€ä¹ˆæ˜¯Abstention Questionsï¼Ÿ
- **å®šä¹‰**: question_idä»¥'_abs'ç»“å°¾çš„é—®é¢˜
- **ç‰¹ç‚¹**: åœ¨haystack_sessionsä¸­**æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆ**ï¼Œç¼ºä¹æ”¯æ’‘è¯æ®
- **æœŸæœ›è¡Œä¸º**: æ¨¡å‹åº”è¯¥æ‹’ç»å›ç­”ï¼ˆå› ä¸ºæ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼‰
- **å®é™…æƒ…å†µ**: æ¨¡å‹**ä¸ä¸€å®šä¼šæ‹’ç­”**ï¼ˆè¿™æ­£æ˜¯æˆ‘ä»¬è¦ç ”ç©¶çš„ï¼ï¼‰

### ä¸ºä»€ä¹ˆåªæµ‹è¯•Abstentionå­é›†ï¼Ÿ
1. **æ›´å¤šæ‹’ç­”æ ·æœ¬**: æ— è¯æ®é—®é¢˜ â†’ æ¨¡å‹æ›´å¯èƒ½æ‹’ç­” â†’ æ›´å¥½è°ƒä¼˜æ£€æµ‹å™¨
2. **èšç„¦æ ¸å¿ƒé—®é¢˜**: é¿å…æ­£å¸¸é—®é¢˜ï¼ˆæœ‰è¯æ®ï¼‰çš„å¹²æ‰°ï¼Œä¸“æ³¨äºæ‹’ç­”æ£€æµ‹

### æ£€æµ‹å™¨è°ƒä¼˜çš„é‡è¦æ€§
- **è¯¯åˆ¤é£é™©**: æ£€æµ‹å™¨å¯èƒ½é”™è¯¯è¯†åˆ«æ­£å¸¸å›ç­”ä¸ºæ‹’ç­”ï¼Œæˆ–æ¼æ‰çœŸå®æ‹’ç­”
- **äººå·¥éªŒè¯**: éœ€è¦äººå·¥æ ‡æ³¨å“ªäº›å›ç­”çœŸçš„æ˜¯æ‹’ç­”ï¼Œç”¨äºè°ƒä¼˜ç®—æ³•
- **ç ”ç©¶ä»·å€¼**: å‡†ç¡®çš„æ‹’ç­”æ£€æµ‹æ˜¯éªŒè¯RLHFè¿‡åº¦ä¿å®ˆç°è±¡çš„åŸºç¡€

## ä¸»è¦åŠŸèƒ½
1. **ä¸“é—¨æµ‹è¯•Abstentionå­é›†** (æ— è¯æ®é—®é¢˜ï¼Œæ‹’ç­”æ ·æœ¬å¤š)
2. **çœŸå®LLMæ¨ç†**: ä½¿ç”¨Qwen2.5 Base/Instructæ¨¡å‹ç”Ÿæˆå›ç­”
3. **æ‹’ç­”æ£€æµ‹**: åº”ç”¨è§„åˆ™åŒ–æ£€æµ‹å™¨åˆ¤æ–­å›ç­”æ˜¯å¦æ‹’ç­”
4. **äººå·¥éªŒè¯æ”¯æŒ**: ä¿å­˜è¯¦ç»†ç»“æœä¾›äººå·¥æ£€æŸ¥å’Œè°ƒä¼˜

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•
```bash
# æµ‹è¯•Instructæ¨¡å‹ï¼ˆé»˜è®¤ï¼Œç»è¿‡RLHFï¼‰
python3 tune_refusal_detector.py

# æµ‹è¯•Baseæ¨¡å‹ï¼ˆæœªç»RLHFï¼‰
python3 tune_refusal_detector.py --model base

# è‡ªå®šä¹‰æµ‹è¯•æ•°é‡
python3 tune_refusal_detector.py --model base --num_test 30
```

### å‚æ•°è¯´æ˜
- `--model/-m`: é€‰æ‹©æ¨¡å‹ç±»å‹
  - `base`: Qwen/Qwen2.5-3B (Baseæ¨¡å‹ï¼Œæœªç»RLHF)
  - `instruct`: Qwen/Qwen2.5-3B-Instruct (Instructæ¨¡å‹ï¼Œç»è¿‡RLHF)
- `--num_test/-n`: æµ‹è¯•å®ä¾‹æ•°é‡ (é»˜è®¤20)

### æµ‹è¯•æµç¨‹
1. **æ•°æ®ç­›é€‰**: åªé€‰æ‹©Abstentioné—®é¢˜ (question_id.endswith('_abs'))
2. **æ¨¡å‹æ¨ç†**: 
   - è¾“å…¥: Question + haystack_sessions (æ— æ­£ç¡®ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡)
   - è¾“å‡º: LLMçš„çœŸå®å›ç­”
3. **æ‹’ç­”æ£€æµ‹**: RefusalDetectoråˆ¤æ–­å›ç­”æ˜¯å¦ä¸ºæ‹’ç­”
4. **ç»“æœä¿å­˜**: ç”ŸæˆJSONæ–‡ä»¶ï¼ŒåŒ…å«äººå·¥æ ‡æ³¨å­—æ®µ

### è¾“å‡ºæ–‡ä»¶
- **æ§åˆ¶å°æŠ¥å‘Š**: å®æ—¶æ˜¾ç¤ºæµ‹è¯•è¿›åº¦å’Œæ£€æµ‹ç»“æœç»Ÿè®¡
- **JSONç»“æœæ–‡ä»¶**: `abstention_test_results_<model>_<num>.json`
  - åŒ…å«æ¯ä¸ªé—®é¢˜çš„å®Œæ•´ä¿¡æ¯ï¼šquestion, response, æ£€æµ‹ç»“æœ
  - åŒ…å«`human_annotation`å­—æ®µä¾›äººå·¥æ ‡æ³¨çœŸå®æ‹’ç­”æƒ…å†µ

### äººå·¥éªŒè¯æµç¨‹
1. **è¿è¡Œè„šæœ¬** â†’ ç”ŸæˆJSONç»“æœæ–‡ä»¶
2. **æ‰“å¼€JSONæ–‡ä»¶** â†’ é€ä¸ªæŸ¥çœ‹æ¨¡å‹å›ç­”
3. **äººå·¥æ ‡æ³¨** â†’ åœ¨`actually_refusal`å­—æ®µå¡«å†™true/false
4. **è°ƒä¼˜æ£€æµ‹å™¨** â†’ æ ¹æ®äººå·¥æ ‡æ³¨ä¼˜åŒ–æ£€æµ‹è§„åˆ™

### é¢„æœŸç»“æœ (RQ2ç ”ç©¶å‡è®¾)
- **Baseæ¨¡å‹**: è¾ƒå°‘æ‹’ç­”ï¼Œå³ä½¿Abstentioné—®é¢˜ä¹Ÿå¯èƒ½å°è¯•å›ç­”
- **Instructæ¨¡å‹**: æ›´å¤šæ‹’ç­”ï¼Œä½“ç°RLHFè®­ç»ƒçš„ä¿å®ˆå€¾å‘
- **æ£€æµ‹å™¨æ€§èƒ½**: é€šè¿‡äººå·¥éªŒè¯ç¡®å®šå‡†ç¡®ç‡ï¼ŒæŒ‡å¯¼åç»­è°ƒä¼˜

## æŠ€æœ¯è¦æ±‚
- **ç¡¬ä»¶**: RTX 4070 (12GBæ˜¾å­˜) æˆ–æ›´å¥½çš„GPU
- **è½¯ä»¶**: PyTorch + CUDA, transformers >= 4.30.0
- **æ•°æ®**: LongMemEval Oracleæ•°æ®é›† (çº¦30ä¸ªAbstentioné—®é¢˜)

## ç ”ç©¶ä»·å€¼
1. **ç®—æ³•éªŒè¯**: ç¡®ä¿æ‹’ç­”æ£€æµ‹å™¨åœ¨çœŸå®åœºæ™¯ä¸­çš„å¯é æ€§
2. **RLHFç ”ç©¶**: ä¸ºéªŒè¯"RLHFå¯¼è‡´è¿‡åº¦ä¿å®ˆ"æä¾›æ£€æµ‹åŸºç¡€
3. **æ–¹æ³•æ”¹è¿›**: é€šè¿‡Abstentionå­é›†çš„å¯†é›†æ‹’ç­”æ ·æœ¬ä¼˜åŒ–æ£€æµ‹ç®—æ³•

æ³¨æ„: æœ¬è„šæœ¬ä¸“é—¨ç”¨äº**è°ƒä¼˜æ‹’ç­”æ£€æµ‹å™¨**ï¼Œä¸ç›´æ¥è¿›è¡ŒRQ2å®éªŒã€‚
RQ2å®éªŒä½¿ç”¨ç»è¿‡æ­¤è„šæœ¬éªŒè¯çš„æ£€æµ‹å™¨è¿›è¡Œå¤§è§„æ¨¡Base vs Instructå¯¹æ¯”ã€‚
"""

import json
import sys
import time
import torch
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass
import re
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from utils.refusal_detector import RefusalDetector
from data.longmemeval_loader import LongMemEvalLoader, LongMemEvalInstance

# æ£€æŸ¥transformersæ˜¯å¦å¯ç”¨
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    question_id: str
    question_type: str
    question: str-
    expected_answer: str
    response: str
    is_refusal: bool
    confidence: float
    is_correct: bool
    matched_patterns: List[str] = None

class RefusalTuner:
    """æ‹’ç­”æ£€æµ‹å™¨è°ƒä¼˜å™¨ - æ”¯æŒçœŸå®æ¨¡å‹æµ‹è¯•"""
    
    def __init__(self, data_path: str = "/mnt/d/datasets/longmemeval_data"):
        self.data_path = data_path
        self.model = None
        self.tokenizer = None
        
    def load_oracle_data(self) -> List[LongMemEvalInstance]:
        """åŠ è½½Oracleæ•°æ®"""
        print("ğŸ“ åŠ è½½LongMemEval Oracleæ•°æ®...")
        oracle_file = f"{self.data_path}/longmemeval_oracle.json"
        self.loader = LongMemEvalLoader(oracle_file)
        instances = self.loader.load_data()
        print(f"âœ… æˆåŠŸåŠ è½½ {len(instances)} ä¸ªå®ä¾‹")
        return instances
    
    def load_model(self, model_name: str):
        """åŠ è½½çœŸå®LLMæ¨¡å‹"""
        if not HAS_TRANSFORMERS:
            raise Exception("transformersåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨çœŸå®æ¨¡å‹")
            
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            
            if torch.cuda.is_available():
                print("ğŸš€ ä½¿ç”¨GPU + FP16æ¨¡å¼")
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map={"": 0},
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                )
            else:
                print("ğŸ–¥ï¸ ä½¿ç”¨CPUæ¨¡å¼")
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def create_prompt(self, instance: LongMemEvalInstance) -> str:
        """åˆ›å»ºæ¨¡å‹è¾“å…¥æç¤º"""
        context_parts = []
        context_parts.append("You are a helpful AI assistant. Below is a conversation history, followed by a question.")
        context_parts.append("\\nConversation History:")
        
        # æ·»åŠ å®Œæ•´ä¼šè¯å†å²
        for i, session in enumerate(instance.haystack_sessions):
            session_text = f"\\n=== Session {i+1} ==="
            for turn in session:
                role = turn.get('role', 'user')
                content = turn.get('content', '')
                turn_text = f"\\n{role.title()}: {content}"
                session_text += turn_text
            context_parts.append(session_text)
        
        # æ·»åŠ é—®é¢˜
        context_parts.append(f"\\n\\nQuestion: {instance.question}")
        context_parts.append("\\nAnswer:")
        
        return "".join(context_parts)
    
    def generate_response(self, instance: LongMemEvalInstance) -> str:
        """ç”¨çœŸå®æ¨¡å‹ç”Ÿæˆå›ç­”"""
        if self.model is None or self.tokenizer is None:
            raise Exception("æ¨¡å‹æœªåŠ è½½")
        
        prompt = self.create_prompt(instance)
        
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", truncation=False)
            
            if torch.cuda.is_available():
                inputs = inputs.to("cuda:0")
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=100,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    num_beams=1,
                    use_cache=True,
                    temperature=None,
                    top_p=None,
                    top_k=None
                )
            
            # è§£ç å›ç­”
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = full_response[len(self.tokenizer.decode(inputs[0], skip_special_tokens=True)):].strip()
            
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return f"[ç”Ÿæˆé”™è¯¯: {e}]"
    
    def create_test_responses(self, instances: List[LongMemEvalInstance]) -> List[Tuple[LongMemEvalInstance, str, bool]]:
        """
        ä¸ºæ¯ä¸ªå®ä¾‹åˆ›å»ºæµ‹è¯•å›å¤
        
        Returns:
            List[Tuple[instance, response, expected_refusal]]
        """
        test_cases = []
        
        for instance in instances:
            # ä¸ºæ¯ä¸ªå®ä¾‹åˆ›å»ºå¤šç§ç±»å‹çš„å›å¤è¿›è¡Œæµ‹è¯•
            
            # 1. æ­£å¸¸å›ç­”ï¼ˆåŸºäºexpected answerï¼‰
            normal_response = f"Based on the conversation history, {instance.answer}"
            test_cases.append((instance, normal_response, False))
            
            # 2. å¼•ç”¨å†å²çš„å›ç­”
            reference_response = f"According to our previous discussion, {instance.answer}"
            test_cases.append((instance, reference_response, False))
            
            # 3. æ˜ç¡®æ‹’ç­”
            explicit_refusal = "I don't have enough information to answer that question."
            test_cases.append((instance, explicit_refusal, True))
            
            # 4. æ— æ³•æ‰¾åˆ°ä¿¡æ¯çš„æ‹’ç­”
            not_found_refusal = "I cannot find that information in our conversation history."
            test_cases.append((instance, not_found_refusal, True))
            
            # 5. ä¸ç¡®å®šçš„å›ç­”
            uncertain_response = f"I'm not sure, but I think {instance.answer}"
            test_cases.append((instance, uncertain_response, True))
            
            # 6. ç®€çŸ­ç›´æ¥å›ç­”
            direct_response = instance.answer
            test_cases.append((instance, direct_response, False))
            
        print(f"ğŸ¯ åˆ›å»ºäº† {len(test_cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
        return test_cases
    
    def test_detector_performance(self, test_cases: List[Tuple[LongMemEvalInstance, str, bool]], 
                                 confidence_threshold: float = 0.7) -> List[TestResult]:
        """æµ‹è¯•æ£€æµ‹å™¨æ€§èƒ½"""
        detector = RefusalDetector(confidence_threshold=confidence_threshold)
        results = []
        
        print(f"ğŸ” æµ‹è¯•æ‹’ç­”æ£€æµ‹å™¨ (é˜ˆå€¼={confidence_threshold})")
        
        for instance, response, expected_refusal in test_cases:
            # è¿›è¡Œæ£€æµ‹
            is_refusal, confidence = detector.detect_refusal(response, instance.question)
            is_correct = (is_refusal == expected_refusal)
            
            # è·å–åŒ¹é…çš„æ¨¡å¼
            analysis = detector.get_detailed_analysis(response)
            matched_patterns = [p['pattern'] for p in analysis.get('matched_patterns', [])]
            
            result = TestResult(
                question_id=instance.question_id,
                question_type=instance.question_type,
                question=instance.question,
                expected_answer=instance.answer,
                response=response,
                is_refusal=is_refusal,
                confidence=confidence,
                is_correct=is_correct,
                matched_patterns=matched_patterns
            )
            
            results.append(result)
        
        return results
    
    def analyze_results(self, results: List[TestResult]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        total = len(results)
        correct = sum(1 for r in results if r.is_correct)
        accuracy = correct / total if total > 0 else 0
        
        # æŒ‰é¢„æœŸç±»å‹åˆ†ç»„
        refusal_results = [r for r in results if ("refusal" in r.response.lower() or 
                                                  "don't" in r.response.lower() or
                                                  "cannot" in r.response.lower() or
                                                  "not sure" in r.response.lower())]
        answer_results = [r for r in results if r not in refusal_results]
        
        # è®¡ç®—å„ç±»å‡†ç¡®ç‡
        refusal_accuracy = sum(1 for r in refusal_results if r.is_correct) / len(refusal_results) if refusal_results else 0
        answer_accuracy = sum(1 for r in answer_results if r.is_correct) / len(answer_results) if answer_results else 0
        
        # æ‰¾å‡ºé”™è¯¯æ¡ˆä¾‹
        errors = [r for r in results if not r.is_correct]
        false_positives = [r for r in errors if r.is_refusal and not ("refusal" in r.response.lower() or 
                                                                      "don't" in r.response.lower() or
                                                                      "cannot" in r.response.lower() or
                                                                      "not sure" in r.response.lower())]
        false_negatives = [r for r in errors if not r.is_refusal and ("refusal" in r.response.lower() or 
                                                                      "don't" in r.response.lower() or
                                                                      "cannot" in r.response.lower() or
                                                                      "not sure" in r.response.lower())]
        
        # ç½®ä¿¡åº¦åˆ†æ
        confidences = [r.confidence for r in results]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        return {
            "total_cases": total,
            "correct_cases": correct,
            "accuracy": accuracy,
            "refusal_accuracy": refusal_accuracy,
            "answer_accuracy": answer_accuracy,
            "false_positives": len(false_positives),
            "false_negatives": len(false_negatives),
            "avg_confidence": avg_confidence,
            "error_cases": errors[:10],  # åªä¿ç•™å‰10ä¸ªé”™è¯¯æ¡ˆä¾‹
            "fp_samples": false_positives[:5],
            "fn_samples": false_negatives[:5]
        }
    
    def test_multiple_thresholds(self, test_cases: List[Tuple[LongMemEvalInstance, str, bool]]) -> Dict[float, Dict]:
        """æµ‹è¯•å¤šä¸ªç½®ä¿¡åº¦é˜ˆå€¼"""
        thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
        results = {}
        
        print("\nğŸ“Š æµ‹è¯•å¤šä¸ªç½®ä¿¡åº¦é˜ˆå€¼...")
        
        for threshold in thresholds:
            print(f"\nğŸ¯ æµ‹è¯•é˜ˆå€¼: {threshold}")
            test_results = self.test_detector_performance(test_cases, threshold)
            analysis = self.analyze_results(test_results)
            results[threshold] = analysis
            
            print(f"  å‡†ç¡®ç‡: {analysis['accuracy']:.3f}")
            print(f"  æ‹’ç­”æ£€æµ‹å‡†ç¡®ç‡: {analysis['refusal_accuracy']:.3f}")
            print(f"  å›ç­”æ£€æµ‹å‡†ç¡®ç‡: {analysis['answer_accuracy']:.3f}")
            print(f"  è¯¯æŠ¥: {analysis['false_positives']}, æ¼æŠ¥: {analysis['false_negatives']}")
        
        return results
    
    def print_detailed_report(self, threshold_results: Dict[float, Dict]):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ” æ‹’ç­”æ£€æµ‹å™¨æ€§èƒ½è°ƒä¼˜æŠ¥å‘Š")
        print("="*80)
        
        # æ‰¾å‡ºæœ€ä½³é˜ˆå€¼
        best_threshold = max(threshold_results.keys(), 
                           key=lambda t: threshold_results[t]['accuracy'])
        best_result = threshold_results[best_threshold]
        
        print(f"\nğŸ† æœ€ä½³é˜ˆå€¼: {best_threshold}")
        print(f"ğŸ“Š æœ€ä½³æ€§èƒ½:")
        print(f"   æ€»ä½“å‡†ç¡®ç‡: {best_result['accuracy']:.3f}")
        print(f"   æ‹’ç­”æ£€æµ‹å‡†ç¡®ç‡: {best_result['refusal_accuracy']:.3f}")
        print(f"   å›ç­”æ£€æµ‹å‡†ç¡®ç‡: {best_result['answer_accuracy']:.3f}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {best_result['avg_confidence']:.3f}")
        
        # é˜ˆå€¼å¯¹æ¯”è¡¨
        print(f"\nğŸ“ˆ é˜ˆå€¼å¯¹æ¯”:")
        print(f"{'é˜ˆå€¼':<6} {'å‡†ç¡®ç‡':<8} {'æ‹’ç­”å‡†ç¡®ç‡':<10} {'å›ç­”å‡†ç¡®ç‡':<10} {'è¯¯æŠ¥':<6} {'æ¼æŠ¥':<6}")
        print("-" * 60)
        for threshold in sorted(threshold_results.keys()):
            result = threshold_results[threshold]
            print(f"{threshold:<6.1f} {result['accuracy']:<8.3f} {result['refusal_accuracy']:<10.3f} "
                  f"{result['answer_accuracy']:<10.3f} {result['false_positives']:<6} {result['false_negatives']:<6}")
        
        # é”™è¯¯æ¡ˆä¾‹åˆ†æ
        if best_result['error_cases']:
            print(f"\nâŒ é”™è¯¯æ¡ˆä¾‹åˆ†æ (å‰5ä¸ª):")
            for i, error in enumerate(best_result['error_cases'][:5], 1):
                expected = "æ‹’ç­”" if ("refusal" in error.response.lower() or 
                                    "don't" in error.response.lower() or
                                    "cannot" in error.response.lower() or
                                    "not sure" in error.response.lower()) else "å›ç­”"
                detected = "æ‹’ç­”" if error.is_refusal else "å›ç­”"
                print(f"\n  {i}. é—®é¢˜ç±»å‹: {error.question_type}")
                print(f"     é—®é¢˜: {error.question[:80]}...")
                print(f"     å›å¤: {error.response[:100]}...")
                print(f"     é¢„æœŸ: {expected} | æ£€æµ‹: {detected} | ç½®ä¿¡åº¦: {error.confidence:.3f}")
                if error.matched_patterns:
                    print(f"     åŒ¹é…æ¨¡å¼: {', '.join(error.matched_patterns)}")
        
        # å»ºè®®
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if best_result['false_positives'] > best_result['false_negatives']:
            print("   - è¯¯æŠ¥è¾ƒå¤šï¼Œè€ƒè™‘é™ä½æŸäº›æ¨¡å¼çš„ç½®ä¿¡åº¦")
            print("   - æ£€æŸ¥æ˜¯å¦æœ‰æ­£å¸¸å›ç­”è¢«è¯¯åˆ¤ä¸ºæ‹’ç­”")
        elif best_result['false_negatives'] > best_result['false_positives']:
            print("   - æ¼æŠ¥è¾ƒå¤šï¼Œè€ƒè™‘æ·»åŠ æ›´å¤šæ‹’ç­”æ¨¡å¼")
            print("   - æ£€æŸ¥æ˜¯å¦æœ‰æ‹’ç­”æœªè¢«è¯†åˆ«")
        else:
            print("   - æ£€æµ‹å™¨æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥ç”¨äºæ­£å¼å®éªŒ")
        
        if best_result['accuracy'] >= 0.95:
            print("   ğŸ‰ æ€§èƒ½ä¼˜ç§€ï¼å¯ä»¥ç›´æ¥ç”¨äºRQ2å®éªŒ")
        elif best_result['accuracy'] >= 0.9:
            print("   âœ… æ€§èƒ½è‰¯å¥½ï¼ŒåŸºæœ¬å¯ä»¥ä½¿ç”¨")
        else:
            print("   âš ï¸ æ€§èƒ½éœ€è¦æ”¹è¿›ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒä¼˜")

def test_abstention_with_real_model(model_name: str = "Qwen/Qwen2.5-3B-Instruct", num_test: int = 20):
    """ç”¨çœŸå®æ¨¡å‹æµ‹è¯•Abstentioné—®é¢˜"""
    print("ğŸ¯ æ‹’ç­”æ£€æµ‹å™¨è°ƒä¼˜ - Abstention Set + çœŸå®æ¨¡å‹")
    print("="*60)
    
    # åˆå§‹åŒ–è°ƒä¼˜å™¨
    tuner = RefusalTuner()
    
    # åŠ è½½æ¨¡å‹
    if not tuner.load_model(model_name):
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
    
    # åŠ è½½æ•°æ®
    instances = tuner.load_oracle_data()
    
    # ç­›é€‰Abstentioné—®é¢˜ - åªæµ‹è¯•è¿™ä¸ªå­é›†
    abstention_instances = [inst for inst in instances if inst.question_id.endswith('_abs')]
    
    print(f"\\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   Abstentioné—®é¢˜æ€»æ•°: {len(abstention_instances)}")
    print(f"   ğŸ’¡ åªæµ‹è¯•Abstentioné—®é¢˜ (æ— è¯æ®â†’æ›´å¤šæ‹’ç­”æ ·æœ¬â†’æ›´å¥½è°ƒä¼˜æ£€æµ‹å™¨)")
    
    if len(abstention_instances) == 0:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°Abstentioné—®é¢˜ï¼Œæ£€æŸ¥æ•°æ®...")
        return
    
    # é€‰æ‹©æµ‹è¯•å®ä¾‹ - åªè¦Abstention
    test_abstention = abstention_instances[:min(num_test, len(abstention_instances))]
    
    print(f"\\nğŸ¯ æµ‹è¯•è®¡åˆ’:")
    print(f"   æµ‹è¯• Abstention é—®é¢˜: {len(test_abstention)}ä¸ª")
    print(f"   è·³è¿‡æ­£å¸¸é—®é¢˜ (æœ‰è¯æ®ï¼Œæ‹’ç­”å°‘ï¼Œä¸é€‚åˆè°ƒä¼˜)")
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = RefusalDetector(confidence_threshold=0.7)
    
    results = []
    
    # æµ‹è¯•Abstentioné—®é¢˜
    print(f"\\nğŸ” æµ‹è¯•Abstentioné—®é¢˜ (åº”è¯¥è¢«æ£€æµ‹ä¸ºæ‹’ç­”):")
    for instance in tqdm(test_abstention, desc="Abstentioné—®é¢˜"):
        try:
            # ç”ŸæˆçœŸå®å›ç­”
            response = tuner.generate_response(instance)
            
            # æ£€æµ‹æ‹’ç­”
            is_refusal, confidence = detector.detect_refusal(
                response=response,
                question=instance.question
            )
            
            # æ³¨æ„ï¼šAbstentioné—®é¢˜è¡¨ç¤ºç¼ºä¹è¯æ®ï¼Œä½†æ¨¡å‹ä¸ä¸€å®šä¼šæ‹’ç­”
            
            results.append({
                "question_id": instance.question_id,
                "question_type": instance.question_type,
                "question": instance.question,  # ä¿ç•™å®Œæ•´é—®é¢˜
                "response": response,
                "is_abstention": True,
                "detected_as_refusal": is_refusal,
                "confidence": confidence,
                "human_annotation": {
                    "actually_refusal": None,  # äººå·¥åˆ¤æ–­ï¼šæ˜¯å¦çœŸçš„æ˜¯æ‹’ç­” (true/false/null)
                    "notes": ""              # äººå·¥å¤‡æ³¨
                }
            })
            
            print(f"   ğŸ“ {instance.question_id}: æ£€æµ‹æ‹’ç­”={is_refusal}, ç½®ä¿¡åº¦={confidence:.3f}")
            
        except Exception as e:
            print(f"   âŒ {instance.question_id}: ç”Ÿæˆå¤±è´¥ - {e}")
    
    # ä¸æµ‹è¯•æ­£å¸¸é—®é¢˜ - ä¸“æ³¨äºAbstentionå­é›†è°ƒä¼˜
    # ç®€å•ç»Ÿè®¡
    detected_refusal = sum(1 for r in results if r['detected_as_refusal'])
    
    print(f"\\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   æµ‹è¯•çš„Abstentioné—®é¢˜: {len(results)}ä¸ª")
    print(f"   æ£€æµ‹å™¨åˆ¤å®šä¸ºæ‹’ç­”: {detected_refusal}ä¸ª")
    print(f"   æ£€æµ‹å™¨åˆ¤å®šä¸ºæ­£å¸¸: {len(results) - detected_refusal}ä¸ª")
    print(f"   âš ï¸  éœ€è¦äººå·¥éªŒè¯å“ªäº›çœŸçš„æ˜¯æ‹’ç­” (è°ƒä¼˜æ£€æµ‹å™¨ç”¨)")
    
    # ä¿å­˜ç»“æœ
    output_file = f"abstention_test_results_{model_name.replace('/', '_')}_{len(results)}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "model_name": model_name,
            "total_tested": len(results),
            "detector_threshold": 0.7,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "human_annotation_guide": {
                "actually_refusal": "äººå·¥åˆ¤æ–­æ¨¡å‹å›ç­”æ˜¯å¦çœŸçš„æ˜¯æ‹’ç­” (true=æ‹’ç­”, false=æ­£å¸¸å›ç­”, null=æœªæ£€æŸ¥)",
                "notes": "äººå·¥å¤‡æ³¨ï¼Œè®°å½•åˆ¤æ–­ç†ç”±æˆ–ç‰¹æ®Šæƒ…å†µ"
            },
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("\\nğŸ“ äººå·¥æ ‡æ³¨è¯´æ˜:")
    print("   1. æ‰“å¼€JSONæ–‡ä»¶ï¼ŒæŸ¥çœ‹æ¯ä¸ªç»“æœçš„'response'å†…å®¹")
    print("   2. åœ¨'human_annotation'å­—æ®µä¸­å¡«å†™æ‚¨çš„åˆ¤æ–­:")
    print("      - actually_refusal: true(æ‹’ç­”) / false(æ­£å¸¸å›ç­”)")
    print("      - notes: è®°å½•åˆ¤æ–­ç†ç”±")
    print("   3. æ ‡æ³¨å®Œæˆåå¯ä»¥ç»Ÿè®¡æ£€æµ‹å™¨å‡†ç¡®ç‡")
    print("ğŸ æµ‹è¯•å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Abstentioné—®é¢˜æ‹’ç­”æ£€æµ‹æµ‹è¯•")
    parser.add_argument("--model", "-m", 
                       choices=["base", "instruct"], 
                       default="instruct",
                       help="é€‰æ‹©æ¨¡å‹ç±»å‹: base(Qwen2.5-3B) æˆ– instruct(Qwen2.5-3B-Instruct)")
    parser.add_argument("--num_test", "-n", 
                       type=int, 
                       default=20,
                       help="æµ‹è¯•å®ä¾‹æ•°é‡")
    
    args = parser.parse_args()
    
    # æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å‹
    if args.model == "base":
        model_name = "Qwen/Qwen2.5-3B"
        print("ğŸ¯ ä½¿ç”¨Baseæ¨¡å‹è¿›è¡Œæµ‹è¯•")
    else:
        model_name = "Qwen/Qwen2.5-3B-Instruct"
        print("ğŸ¯ ä½¿ç”¨Instructæ¨¡å‹è¿›è¡Œæµ‹è¯•")
    
    # æµ‹è¯•Abstentioné—®é¢˜
    test_abstention_with_real_model(
        model_name=model_name,
        num_test=args.num_test
    )

if __name__ == "__main__":
    main()
